{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## background \n",
    "- 32% survival\n",
    "- women,children,upper class more likely survived\n",
    "- not enough boats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler,Imputer\n",
    "from util import *\n",
    "from onehot import LabelBinarizerEx\n",
    "from pipeline import FeaturePipeline, DataFramePipeline\n",
    "from binning import Binner\n",
    "from title import TitleExtractor\n",
    "from cabin import HasCabin\n",
    "from ensemble import EnsembleStackClassifierEx\n",
    "from addcols import AddColumns\n",
    "from impute import GroupImputer,MixImputer\n",
    "from alone import IsAlone\n",
    "from scipy.stats import boxcox\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "passenger_train=pd.read_csv('raw_data/train.csv')\n",
    "passenger_test=pd.read_csv('raw_data/test.csv')\n",
    "target_col='Survived'\n",
    "id_col='PassengerId'\n",
    "target=passenger_train[target_col]\n",
    "total_num=len(passenger_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>714.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>2.308642</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>0.523008</td>\n",
       "      <td>0.381594</td>\n",
       "      <td>32.204208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>257.353842</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.836071</td>\n",
       "      <td>14.526497</td>\n",
       "      <td>1.102743</td>\n",
       "      <td>0.806057</td>\n",
       "      <td>49.693429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>223.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>20.125000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>446.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>668.500000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
       "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
       "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
       "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
       "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
       "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
       "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
       "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
       "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
       "\n",
       "            Parch        Fare  \n",
       "count  891.000000  891.000000  \n",
       "mean     0.381594   32.204208  \n",
       "std      0.806057   49.693429  \n",
       "min      0.000000    0.000000  \n",
       "25%      0.000000    7.910400  \n",
       "50%      0.000000   14.454200  \n",
       "75%      0.000000   31.000000  \n",
       "max      6.000000  512.329200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>332.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>418.000000</td>\n",
       "      <td>417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>2.265550</td>\n",
       "      <td>30.272590</td>\n",
       "      <td>0.447368</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>35.627188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>120.810458</td>\n",
       "      <td>0.841838</td>\n",
       "      <td>14.181209</td>\n",
       "      <td>0.896760</td>\n",
       "      <td>0.981429</td>\n",
       "      <td>55.907576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>892.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.170000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>996.250000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>21.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.895800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1100.500000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>27.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.454200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1204.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>39.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>31.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1309.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>512.329200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       PassengerId      Pclass         Age       SibSp       Parch        Fare\n",
       "count   418.000000  418.000000  332.000000  418.000000  418.000000  417.000000\n",
       "mean   1100.500000    2.265550   30.272590    0.447368    0.392344   35.627188\n",
       "std     120.810458    0.841838   14.181209    0.896760    0.981429   55.907576\n",
       "min     892.000000    1.000000    0.170000    0.000000    0.000000    0.000000\n",
       "25%     996.250000    1.000000   21.000000    0.000000    0.000000    7.895800\n",
       "50%    1100.500000    3.000000   27.000000    0.000000    0.000000   14.454200\n",
       "75%    1204.750000    3.000000   39.000000    1.000000    0.000000   31.500000\n",
       "max    1309.000000    3.000000   76.000000    8.000000    9.000000  512.329200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>891</td>\n",
       "      <td>204</td>\n",
       "      <td>889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>891</td>\n",
       "      <td>2</td>\n",
       "      <td>681</td>\n",
       "      <td>147</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Hirvonen, Miss. Hildur E</td>\n",
       "      <td>male</td>\n",
       "      <td>347082</td>\n",
       "      <td>G6</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>577</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>644</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Name   Sex  Ticket Cabin Embarked\n",
       "count                        891   891     891   204      889\n",
       "unique                       891     2     681   147        3\n",
       "top     Hirvonen, Miss. Hildur E  male  347082    G6        S\n",
       "freq                           1   577       7     4      644"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger_train.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>418</td>\n",
       "      <td>418</td>\n",
       "      <td>418</td>\n",
       "      <td>91</td>\n",
       "      <td>418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>418</td>\n",
       "      <td>2</td>\n",
       "      <td>363</td>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>Elias, Mr. Joseph</td>\n",
       "      <td>male</td>\n",
       "      <td>PC 17608</td>\n",
       "      <td>B57 B59 B63 B66</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>266</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Name   Sex    Ticket            Cabin Embarked\n",
       "count                 418   418       418               91      418\n",
       "unique                418     2       363               76        3\n",
       "top     Elias, Mr. Joseph  male  PC 17608  B57 B59 B63 B66        S\n",
       "freq                    1   266         5                3      270"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "passenger_test.describe(include=['O'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['IsAlone', 'Pclass_Norm', 'Age_Norm', 'Fare_Norm', 'SibSp_Norm',\n",
       "       'Parch_Norm', 'FamilySize_Norm', 'Cabins_Norm', 'HasCabin',\n",
       "       'Sex_female', 'Sex_male', 'Title_Master', 'Title_Miss', 'Title_Mr',\n",
       "       'Title_Mrs', 'Embarked_Imp_C', 'Embarked_Imp_Q', 'Embarked_Imp_S'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df=pd.concat([passenger_train.drop(target_col,axis=1),passenger_test]).drop(id_col,axis=1)\n",
    "raw_df.index=np.arange(len(raw_df))\n",
    "org_cols=raw_df.columns\n",
    "\n",
    "age_group=raw_df.groupby(['Pclass','Sex']).mean()['Age']\n",
    "missing_index=raw_df[raw_df.Age.isnull()].index\n",
    "for i in missing_index:\n",
    "    raw_df.loc[i,'Age'] = age_group[(raw_df.loc[i,'Pclass'],raw_df.loc[i,'Sex'])]\n",
    "    \n",
    "raw_df.Fare=Imputer(strategy='mean').fit_transform(raw_df.Fare.values.reshape(-1,1))\n",
    "\n",
    "raw_df['FamilySize']=raw_df.SibSp+raw_df.Parch\n",
    "\n",
    "raw_df['IsAlone']=(raw_df.FamilySize==0).astype('int')\n",
    "\n",
    "def countCabin(cabin):\n",
    "    c=0\n",
    "    if cabin is not np.nan:\n",
    "        c=len(cabin.split())\n",
    "    return c\n",
    "\n",
    "raw_df['Cabins']=raw_df.Cabin.apply(countCabin)\n",
    "\n",
    "for c in ['Pclass','Age','Fare','SibSp','Parch','FamilySize','Cabins']:\n",
    "    new_c=c+'_Norm'\n",
    "    raw_df[new_c]=raw_df[c]\n",
    "    if raw_df[new_c].min()<=0.:\n",
    "        raw_df[new_c]=raw_df[new_c]+abs(raw_df[new_c].min())+0.1\n",
    "    tranformed,_=boxcox(raw_df[new_c])\n",
    "    raw_df[new_c]=StandardScaler().fit_transform(tranformed.reshape(-1,1))\n",
    "    raw_df.drop(c,axis=1,inplace=True)\n",
    "import re\n",
    "\n",
    "# def extractTitle(name):\n",
    "#     m = re.search(' \\w+\\\\.',name)\n",
    "#     if m:\n",
    "#         return m.group()[1:-1]\n",
    "#     else:\n",
    "#         return np.nan\n",
    "    \n",
    "def extractTitle(name):\n",
    "    title=np.nan\n",
    "    m = re.search(' \\w+\\\\.',name)\n",
    "    if m:\n",
    "        t=m.group()[1:-1]\n",
    "        if t in ['Mr','Miss','Mrs','Master']:\n",
    "            title = t\n",
    "    return title\n",
    "    \n",
    "raw_df['Title']=raw_df.Name.apply(extractTitle)\n",
    "\n",
    "# def extractTicketNumber(ticket):\n",
    "#     try:\n",
    "#         return float(ticket)\n",
    "#     except:\n",
    "#         splits=ticket.split()\n",
    "#         if len(splits)>1:\n",
    "#             return float(splits[1])\n",
    "#         else:\n",
    "#             return np.nan\n",
    "        \n",
    "# raw_df['Ticket_Number']=raw_df.Ticket.apply(extractTicketNumber)\n",
    "\n",
    "# def extractTicketLocation(ticket):\n",
    "#     m = re.search('\\w+ ',ticket)\n",
    "#     if m:\n",
    "#         return m.group()[0]\n",
    "#     else:\n",
    "#         return np.nan\n",
    "\n",
    "# raw_df['Ticket_Location']=raw_df.Ticket.apply(extractTicketLocation)\n",
    "\n",
    "def isTicketNum(ticket):\n",
    "    try:\n",
    "        int(ticket)\n",
    "        return 1\n",
    "    except Exception as e:\n",
    "        return 0\n",
    "    \n",
    "# raw_df['IsTicketNum']=raw_df.Ticket.apply(isTicketNum)\n",
    "\n",
    "raw_df['HasCabin']=(raw_df.Cabin.isnull()==False).astype('int')\n",
    "\n",
    "raw_df['Embarked_Imp']=MixImputer().fit_transform(raw_df[['Embarked']])\n",
    "\n",
    "raw_df=pd.get_dummies(raw_df,columns=['Sex','Title','Embarked_Imp'])\n",
    "\n",
    "pre_df=raw_df.drop(['Name','Ticket','Cabin','Embarked'],axis=1)\n",
    "pre_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data=pre_df[:len(passenger_train)]\n",
    "test_data=pre_df[len(passenger_train):]\n",
    "len(train_data)==len(passenger_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>IsAlone</th>\n",
       "      <th>Pclass_Norm</th>\n",
       "      <th>Age_Norm</th>\n",
       "      <th>Fare_Norm</th>\n",
       "      <th>SibSp_Norm</th>\n",
       "      <th>Parch_Norm</th>\n",
       "      <th>FamilySize_Norm</th>\n",
       "      <th>Cabins_Norm</th>\n",
       "      <th>HasCabin</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>Title_Master</th>\n",
       "      <th>Title_Miss</th>\n",
       "      <th>Title_Mr</th>\n",
       "      <th>Title_Mrs</th>\n",
       "      <th>Embarked_Imp_C</th>\n",
       "      <th>Embarked_Imp_Q</th>\n",
       "      <th>Embarked_Imp_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "      <td>891.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.602694</td>\n",
       "      <td>0.017178</td>\n",
       "      <td>-0.007725</td>\n",
       "      <td>-0.021887</td>\n",
       "      <td>-0.002092</td>\n",
       "      <td>0.010635</td>\n",
       "      <td>0.002969</td>\n",
       "      <td>0.008422</td>\n",
       "      <td>0.228956</td>\n",
       "      <td>0.352413</td>\n",
       "      <td>0.647587</td>\n",
       "      <td>0.044893</td>\n",
       "      <td>0.204265</td>\n",
       "      <td>0.580247</td>\n",
       "      <td>0.140292</td>\n",
       "      <td>0.188552</td>\n",
       "      <td>0.086420</td>\n",
       "      <td>0.725028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.489615</td>\n",
       "      <td>0.998623</td>\n",
       "      <td>1.013282</td>\n",
       "      <td>1.013225</td>\n",
       "      <td>1.001519</td>\n",
       "      <td>1.007094</td>\n",
       "      <td>1.002780</td>\n",
       "      <td>1.005835</td>\n",
       "      <td>0.420397</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.477990</td>\n",
       "      <td>0.207186</td>\n",
       "      <td>0.403390</td>\n",
       "      <td>0.493796</td>\n",
       "      <td>0.347485</td>\n",
       "      <td>0.391372</td>\n",
       "      <td>0.281141</td>\n",
       "      <td>0.446751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.472855</td>\n",
       "      <td>-2.699889</td>\n",
       "      <td>-3.958510</td>\n",
       "      <td>-0.684350</td>\n",
       "      <td>-0.553478</td>\n",
       "      <td>-0.805476</td>\n",
       "      <td>-0.539368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.509118</td>\n",
       "      <td>-0.509589</td>\n",
       "      <td>-0.753117</td>\n",
       "      <td>-0.684350</td>\n",
       "      <td>-0.553478</td>\n",
       "      <td>-0.805476</td>\n",
       "      <td>-0.539368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869898</td>\n",
       "      <td>-0.196732</td>\n",
       "      <td>-0.230710</td>\n",
       "      <td>-0.684350</td>\n",
       "      <td>-0.553478</td>\n",
       "      <td>-0.805476</td>\n",
       "      <td>-0.539368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869898</td>\n",
       "      <td>0.610768</td>\n",
       "      <td>0.467671</td>\n",
       "      <td>1.419425</td>\n",
       "      <td>-0.553478</td>\n",
       "      <td>1.052411</td>\n",
       "      <td>-0.539368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.869898</td>\n",
       "      <td>3.350701</td>\n",
       "      <td>3.410959</td>\n",
       "      <td>1.657046</td>\n",
       "      <td>1.856903</td>\n",
       "      <td>1.624263</td>\n",
       "      <td>1.891378</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          IsAlone  Pclass_Norm    Age_Norm   Fare_Norm  SibSp_Norm  \\\n",
       "count  891.000000   891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.602694     0.017178   -0.007725   -0.021887   -0.002092   \n",
       "std      0.489615     0.998623    1.013282    1.013225    1.001519   \n",
       "min      0.000000    -1.472855   -2.699889   -3.958510   -0.684350   \n",
       "25%      0.000000    -0.509118   -0.509589   -0.753117   -0.684350   \n",
       "50%      1.000000     0.869898   -0.196732   -0.230710   -0.684350   \n",
       "75%      1.000000     0.869898    0.610768    0.467671    1.419425   \n",
       "max      1.000000     0.869898    3.350701    3.410959    1.657046   \n",
       "\n",
       "       Parch_Norm  FamilySize_Norm  Cabins_Norm    HasCabin  Sex_female  \\\n",
       "count  891.000000       891.000000   891.000000  891.000000  891.000000   \n",
       "mean     0.010635         0.002969     0.008422    0.228956    0.352413   \n",
       "std      1.007094         1.002780     1.005835    0.420397    0.477990   \n",
       "min     -0.553478        -0.805476    -0.539368    0.000000    0.000000   \n",
       "25%     -0.553478        -0.805476    -0.539368    0.000000    0.000000   \n",
       "50%     -0.553478        -0.805476    -0.539368    0.000000    0.000000   \n",
       "75%     -0.553478         1.052411    -0.539368    0.000000    1.000000   \n",
       "max      1.856903         1.624263     1.891378    1.000000    1.000000   \n",
       "\n",
       "         Sex_male  Title_Master  Title_Miss    Title_Mr   Title_Mrs  \\\n",
       "count  891.000000    891.000000  891.000000  891.000000  891.000000   \n",
       "mean     0.647587      0.044893    0.204265    0.580247    0.140292   \n",
       "std      0.477990      0.207186    0.403390    0.493796    0.347485   \n",
       "min      0.000000      0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000      0.000000    0.000000    0.000000    0.000000   \n",
       "50%      1.000000      0.000000    0.000000    1.000000    0.000000   \n",
       "75%      1.000000      0.000000    0.000000    1.000000    0.000000   \n",
       "max      1.000000      1.000000    1.000000    1.000000    1.000000   \n",
       "\n",
       "       Embarked_Imp_C  Embarked_Imp_Q  Embarked_Imp_S  \n",
       "count      891.000000      891.000000      891.000000  \n",
       "mean         0.188552        0.086420        0.725028  \n",
       "std          0.391372        0.281141        0.446751  \n",
       "min          0.000000        0.000000        0.000000  \n",
       "25%          0.000000        0.000000        0.000000  \n",
       "50%          0.000000        0.000000        1.000000  \n",
       "75%          0.000000        0.000000        1.000000  \n",
       "max          1.000000        1.000000        1.000000  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survived           1.000000\n",
       "Sex_female         0.543351\n",
       "Title_Mrs          0.339040\n",
       "Title_Miss         0.327093\n",
       "Fare_Norm          0.320527\n",
       "HasCabin           0.316912\n",
       "Cabins_Norm        0.316763\n",
       "FamilySize_Norm    0.187595\n",
       "Embarked_Imp_C     0.168240\n",
       "Parch_Norm         0.146692\n",
       "SibSp_Norm         0.108983\n",
       "Title_Master       0.085221\n",
       "Embarked_Imp_Q     0.003650\n",
       "Age_Norm          -0.073732\n",
       "Embarked_Imp_S    -0.149683\n",
       "IsAlone           -0.203367\n",
       "Pclass_Norm       -0.339811\n",
       "Sex_male          -0.543351\n",
       "Title_Mr          -0.549199\n",
       "Name: Survived, dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp=train_data.copy()\n",
    "tmp['Survived']=target\n",
    "tmp.corr()['Survived'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "last_result={}\n",
    "from imp import reload\n",
    "from aml import auto_model_machine as aml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-18 12:05:17,376 - begin [0] for LogisticRegression {C = 0.001, penalty = l1}\n",
      "2017-11-18 12:05:17,376 - score 0.616 time 0\n",
      "2017-11-18 12:05:17,376 - begin [1] for LogisticRegression {C = 0.001, penalty = l2}\n",
      "2017-11-18 12:05:17,393 - score 0.751 time 0\n",
      "2017-11-18 12:05:17,394 - begin [2] for LogisticRegression {C = 0.01, penalty = l1}\n",
      "2017-11-18 12:05:17,396 - score 0.789 time 0\n",
      "2017-11-18 12:05:17,398 - begin [3] for LogisticRegression {C = 0.01, penalty = l2}\n",
      "2017-11-18 12:05:17,400 - score 0.789 time 0\n",
      "2017-11-18 12:05:17,402 - begin [4] for LogisticRegression {C = 0.1, penalty = l1}\n",
      "2017-11-18 12:05:17,406 - score 0.796 time 0\n",
      "2017-11-18 12:05:17,408 - begin [5] for LogisticRegression {C = 0.1, penalty = l2}\n",
      "2017-11-18 12:05:17,410 - score 0.814 time 0\n",
      "2017-11-18 12:05:17,412 - begin [6] for LogisticRegression {C = 1, penalty = l1}\n",
      "2017-11-18 12:05:17,415 - score 0.823 time 0\n",
      "2017-11-18 12:05:17,417 - begin [7] for LogisticRegression {C = 1, penalty = l2}\n",
      "2017-11-18 12:05:17,421 - score 0.816 time 0\n",
      "2017-11-18 12:05:17,422 - begin [8] for LogisticRegression {C = 10, penalty = l1}\n",
      "2017-11-18 12:05:17,429 - score 0.825 time 0\n",
      "2017-11-18 12:05:17,432 - begin [9] for LogisticRegression {C = 10, penalty = l2}\n",
      "2017-11-18 12:05:17,440 - score 0.819 time 0\n",
      "2017-11-18 12:05:17,442 - begin [10] for LogisticRegression {C = 100, penalty = l1}\n",
      "2017-11-18 12:05:17,454 - score 0.828 time 0\n",
      "2017-11-18 12:05:17,456 - begin [11] for LogisticRegression {C = 100, penalty = l2}\n",
      "2017-11-18 12:05:17,459 - score 0.828 time 0\n",
      "2017-11-18 12:05:17,461 - begin [12] for LogisticRegression {C = 1000, penalty = l1}\n",
      "2017-11-18 12:05:17,465 - score 0.828 time 0\n",
      "2017-11-18 12:05:17,467 - begin [13] for LogisticRegression {C = 1000, penalty = l2}\n",
      "2017-11-18 12:05:17,470 - score 0.828 time 0\n",
      "2017-11-18 12:05:17,473 - begin [14] for RandomForestClassifier {max_depth = None, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:17,492 - score 0.807 time 0\n",
      "2017-11-18 12:05:17,498 - begin [15] for RandomForestClassifier {max_depth = None, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:17,510 - score 0.825 time 0\n",
      "2017-11-18 12:05:17,511 - begin [16] for RandomForestClassifier {max_depth = None, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:17,528 - score 0.820 time 0\n",
      "2017-11-18 12:05:17,530 - begin [17] for RandomForestClassifier {max_depth = None, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:17,547 - score 0.818 time 0\n",
      "2017-11-18 12:05:17,550 - begin [18] for RandomForestClassifier {max_depth = None, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:17,568 - score 0.811 time 0\n",
      "2017-11-18 12:05:17,570 - begin [19] for RandomForestClassifier {max_depth = None, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:17,587 - score 0.807 time 0\n",
      "2017-11-18 12:05:17,594 - begin [20] for RandomForestClassifier {max_depth = None, max_features = sqrt, n_estimators = 100}\n",
      "2017-11-18 12:05:17,606 - score 0.811 time 0\n",
      "2017-11-18 12:05:17,608 - begin [21] for RandomForestClassifier {max_depth = None, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:17,632 - score 0.805 time 0\n",
      "2017-11-18 12:05:17,633 - begin [22] for RandomForestClassifier {max_depth = 3, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:17,648 - score 0.813 time 0\n",
      "2017-11-18 12:05:17,651 - begin [23] for RandomForestClassifier {max_depth = 3, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:17,668 - score 0.824 time 0\n",
      "2017-11-18 12:05:17,671 - begin [24] for RandomForestClassifier {max_depth = 3, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:17,687 - score 0.824 time 0\n",
      "2017-11-18 12:05:17,688 - begin [25] for RandomForestClassifier {max_depth = 3, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:17,706 - score 0.825 time 0\n",
      "2017-11-18 12:05:17,709 - begin [26] for RandomForestClassifier {max_depth = 3, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:17,726 - score 0.817 time 0\n",
      "2017-11-18 12:05:17,728 - begin [27] for RandomForestClassifier {max_depth = 3, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:17,750 - score 0.817 time 0\n",
      "2017-11-18 12:05:17,752 - begin [28] for RandomForestClassifier {max_depth = 3, max_features = sqrt, n_estimators = 100}\n",
      "2017-11-18 12:05:17,770 - score 0.819 time 0\n",
      "2017-11-18 12:05:17,778 - begin [29] for RandomForestClassifier {max_depth = 3, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:17,794 - score 0.813 time 0\n",
      "2017-11-18 12:05:17,797 - begin [30] for RandomForestClassifier {max_depth = 5, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:17,818 - score 0.828 time 0\n",
      "2017-11-18 12:05:17,820 - begin [31] for RandomForestClassifier {max_depth = 5, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:17,836 - score 0.831 time 0\n",
      "2017-11-18 12:05:17,838 - begin [32] for RandomForestClassifier {max_depth = 5, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:17,857 - score 0.831 time 0\n",
      "2017-11-18 12:05:17,860 - begin [33] for RandomForestClassifier {max_depth = 5, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:17,881 - score 0.824 time 0\n",
      "2017-11-18 12:05:17,885 - begin [34] for RandomForestClassifier {max_depth = 5, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:17,902 - score 0.831 time 0\n",
      "2017-11-18 12:05:17,905 - begin [35] for RandomForestClassifier {max_depth = 5, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:17,926 - score 0.828 time 0\n",
      "2017-11-18 12:05:17,930 - begin [36] for RandomForestClassifier {max_depth = 5, max_features = sqrt, n_estimators = 100}\n",
      "2017-11-18 12:05:17,945 - score 0.826 time 0\n",
      "2017-11-18 12:05:17,947 - begin [37] for RandomForestClassifier {max_depth = 5, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:17,967 - score 0.832 time 0\n",
      "2017-11-18 12:05:17,969 - begin [38] for RandomForestClassifier {max_depth = 8, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:17,987 - score 0.818 time 0\n",
      "2017-11-18 12:05:17,990 - begin [39] for RandomForestClassifier {max_depth = 8, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:18,012 - score 0.835 time 0\n",
      "2017-11-18 12:05:18,014 - begin [40] for RandomForestClassifier {max_depth = 8, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:18,034 - score 0.828 time 0\n",
      "2017-11-18 12:05:18,038 - begin [41] for RandomForestClassifier {max_depth = 8, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:18,058 - score 0.832 time 0\n",
      "2017-11-18 12:05:18,063 - begin [42] for RandomForestClassifier {max_depth = 8, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:18,083 - score 0.828 time 0\n",
      "2017-11-18 12:05:18,085 - begin [43] for RandomForestClassifier {max_depth = 8, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:18,104 - score 0.832 time 0\n",
      "2017-11-18 12:05:18,111 - begin [44] for RandomForestClassifier {max_depth = 8, max_features = sqrt, n_estimators = 100}\n",
      "2017-11-18 12:05:18,133 - score 0.828 time 0\n",
      "2017-11-18 12:05:18,135 - begin [45] for RandomForestClassifier {max_depth = 8, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:18,156 - score 0.822 time 0\n",
      "2017-11-18 12:05:18,158 - begin [46] for RandomForestClassifier {max_depth = 15, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:18,179 - score 0.803 time 0\n",
      "2017-11-18 12:05:18,181 - begin [47] for RandomForestClassifier {max_depth = 15, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:18,203 - score 0.820 time 0\n",
      "2017-11-18 12:05:18,205 - begin [48] for RandomForestClassifier {max_depth = 15, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:18,228 - score 0.823 time 0\n",
      "2017-11-18 12:05:18,231 - begin [49] for RandomForestClassifier {max_depth = 15, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:18,252 - score 0.817 time 0\n",
      "2017-11-18 12:05:18,254 - begin [50] for RandomForestClassifier {max_depth = 15, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:18,282 - score 0.813 time 0\n",
      "2017-11-18 12:05:18,284 - begin [51] for RandomForestClassifier {max_depth = 15, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:18,314 - score 0.814 time 0\n",
      "2017-11-18 12:05:18,319 - begin [52] for RandomForestClassifier {max_depth = 15, max_features = sqrt, n_estimators = 100}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-18 12:05:18,351 - score 0.820 time 0\n",
      "2017-11-18 12:05:18,353 - begin [53] for RandomForestClassifier {max_depth = 15, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:18,375 - score 0.818 time 0\n",
      "2017-11-18 12:05:18,377 - begin [54] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:18,405 - score 0.818 time 0\n",
      "2017-11-18 12:05:18,407 - begin [55] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:18,433 - score 0.820 time 0\n",
      "2017-11-18 12:05:18,436 - begin [56] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:18,469 - score 0.811 time 0\n",
      "2017-11-18 12:05:18,471 - begin [57] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:18,489 - score 0.823 time 0\n",
      "2017-11-18 12:05:18,493 - begin [58] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:18,511 - score 0.825 time 0\n",
      "2017-11-18 12:05:18,515 - begin [59] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:18,553 - score 0.827 time 0\n",
      "2017-11-18 12:05:18,556 - begin [60] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:18,594 - score 0.824 time 0\n",
      "2017-11-18 12:05:18,600 - begin [61] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:18,634 - score 0.838 time 0\n",
      "2017-11-18 12:05:18,638 - begin [62] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = None, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:18,666 - score 0.829 time 0\n",
      "2017-11-18 12:05:18,668 - begin [63] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:18,694 - score 0.816 time 0\n",
      "2017-11-18 12:05:18,697 - begin [64] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:18,724 - score 0.815 time 0\n",
      "2017-11-18 12:05:18,733 - begin [65] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:18,760 - score 0.816 time 0\n",
      "2017-11-18 12:05:18,767 - begin [66] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:18,796 - score 0.825 time 0\n",
      "2017-11-18 12:05:18,797 - begin [67] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:18,828 - score 0.834 time 0\n",
      "2017-11-18 12:05:18,830 - begin [68] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:18,864 - score 0.831 time 0\n",
      "2017-11-18 12:05:18,866 - begin [69] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:18,900 - score 0.828 time 0\n",
      "2017-11-18 12:05:18,902 - begin [70] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:18,932 - score 0.826 time 0\n",
      "2017-11-18 12:05:18,934 - begin [71] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 3, max_features = sqrt, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:18,956 - score 0.831 time 0\n",
      "2017-11-18 12:05:18,960 - begin [72] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:18,988 - score 0.813 time 0\n",
      "2017-11-18 12:05:18,998 - begin [73] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:19,030 - score 0.822 time 0\n",
      "2017-11-18 12:05:19,032 - begin [74] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:19,064 - score 0.829 time 0\n",
      "2017-11-18 12:05:19,066 - begin [75] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:19,097 - score 0.831 time 0\n",
      "2017-11-18 12:05:19,099 - begin [76] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:19,128 - score 0.837 time 0\n",
      "2017-11-18 12:05:19,130 - begin [77] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:19,161 - score 0.834 time 0\n",
      "2017-11-18 12:05:19,167 - begin [78] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:19,200 - score 0.834 time 0\n",
      "2017-11-18 12:05:19,203 - begin [79] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:19,237 - score 0.825 time 0\n",
      "2017-11-18 12:05:19,239 - begin [80] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = None, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:19,270 - score 0.823 time 0\n",
      "2017-11-18 12:05:19,272 - begin [81] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:19,310 - score 0.828 time 0\n",
      "2017-11-18 12:05:19,312 - begin [82] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:19,348 - score 0.826 time 0\n",
      "2017-11-18 12:05:19,353 - begin [83] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:19,379 - score 0.828 time 0\n",
      "2017-11-18 12:05:19,384 - begin [84] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:19,411 - score 0.828 time 0\n",
      "2017-11-18 12:05:19,414 - begin [85] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:19,444 - score 0.832 time 0\n",
      "2017-11-18 12:05:19,447 - begin [86] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:19,479 - score 0.827 time 0\n",
      "2017-11-18 12:05:19,481 - begin [87] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:19,510 - score 0.835 time 0\n",
      "2017-11-18 12:05:19,512 - begin [88] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:19,543 - score 0.834 time 0\n",
      "2017-11-18 12:05:19,545 - begin [89] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 5, max_features = sqrt, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:19,577 - score 0.829 time 0\n",
      "2017-11-18 12:05:19,579 - begin [90] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:19,604 - score 0.815 time 0\n",
      "2017-11-18 12:05:19,606 - begin [91] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:19,638 - score 0.831 time 0\n",
      "2017-11-18 12:05:19,641 - begin [92] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 100, subsample = 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-18 12:05:19,672 - score 0.831 time 0\n",
      "2017-11-18 12:05:19,675 - begin [93] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:19,705 - score 0.814 time 0\n",
      "2017-11-18 12:05:19,708 - begin [94] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:19,742 - score 0.817 time 0\n",
      "2017-11-18 12:05:19,743 - begin [95] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:19,772 - score 0.818 time 0\n",
      "2017-11-18 12:05:19,774 - begin [96] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:19,804 - score 0.809 time 0\n",
      "2017-11-18 12:05:19,809 - begin [97] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:19,839 - score 0.814 time 0\n",
      "2017-11-18 12:05:19,845 - begin [98] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = None, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:19,872 - score 0.824 time 0\n",
      "2017-11-18 12:05:19,882 - begin [99] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:19,913 - score 0.819 time 0\n",
      "2017-11-18 12:05:19,916 - begin [100] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:19,948 - score 0.831 time 0\n",
      "2017-11-18 12:05:19,950 - begin [101] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:19,979 - score 0.829 time 0\n",
      "2017-11-18 12:05:19,981 - begin [102] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:20,012 - score 0.819 time 0\n",
      "2017-11-18 12:05:20,014 - begin [103] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:20,046 - score 0.822 time 0\n",
      "2017-11-18 12:05:20,050 - begin [104] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:20,084 - score 0.826 time 0\n",
      "2017-11-18 12:05:20,086 - begin [105] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:20,118 - score 0.810 time 0\n",
      "2017-11-18 12:05:20,122 - begin [106] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:20,154 - score 0.811 time 0\n",
      "2017-11-18 12:05:20,157 - begin [107] for GradientBoostingClassifier {learning_rate = 0.01, max_depth = 8, max_features = sqrt, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:20,187 - score 0.809 time 0\n",
      "2017-11-18 12:05:20,191 - begin [108] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:20,222 - score 0.836 time 0\n",
      "2017-11-18 12:05:20,229 - begin [109] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:20,261 - score 0.836 time 0\n",
      "2017-11-18 12:05:20,263 - begin [110] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:20,296 - score 0.836 time 0\n",
      "2017-11-18 12:05:20,299 - begin [111] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:20,333 - score 0.823 time 0\n",
      "2017-11-18 12:05:20,336 - begin [112] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:20,369 - score 0.838 time 0\n",
      "2017-11-18 12:05:20,373 - begin [113] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:20,407 - score 0.832 time 0\n",
      "2017-11-18 12:05:20,410 - begin [114] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:20,440 - score 0.815 time 0\n",
      "2017-11-18 12:05:20,443 - begin [115] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:20,478 - score 0.831 time 0\n",
      "2017-11-18 12:05:20,480 - begin [116] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:20,512 - score 0.834 time 0\n",
      "2017-11-18 12:05:20,514 - begin [117] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:20,552 - score 0.824 time 0\n",
      "2017-11-18 12:05:20,554 - begin [118] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:20,587 - score 0.826 time 0\n",
      "2017-11-18 12:05:20,592 - begin [119] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:20,626 - score 0.829 time 0\n",
      "2017-11-18 12:05:20,630 - begin [120] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:20,661 - score 0.833 time 0\n",
      "2017-11-18 12:05:20,665 - begin [121] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:20,699 - score 0.829 time 0\n",
      "2017-11-18 12:05:20,703 - begin [122] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:20,733 - score 0.833 time 0\n",
      "2017-11-18 12:05:20,740 - begin [123] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:20,776 - score 0.828 time 0\n",
      "2017-11-18 12:05:20,778 - begin [124] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:20,815 - score 0.825 time 0\n",
      "2017-11-18 12:05:20,816 - begin [125] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = sqrt, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:20,849 - score 0.826 time 0\n",
      "2017-11-18 12:05:20,856 - begin [126] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:20,890 - score 0.832 time 0\n",
      "2017-11-18 12:05:20,893 - begin [127] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:20,928 - score 0.829 time 0\n",
      "2017-11-18 12:05:20,930 - begin [128] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:20,963 - score 0.816 time 0\n",
      "2017-11-18 12:05:20,968 - begin [129] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:21,002 - score 0.813 time 0\n",
      "2017-11-18 12:05:21,004 - begin [130] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:21,039 - score 0.816 time 0\n",
      "2017-11-18 12:05:21,041 - begin [131] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:21,078 - score 0.820 time 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-18 12:05:21,083 - begin [132] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:21,117 - score 0.797 time 0\n",
      "2017-11-18 12:05:21,119 - begin [133] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:21,160 - score 0.819 time 0\n",
      "2017-11-18 12:05:21,162 - begin [134] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = None, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:21,199 - score 0.818 time 0\n",
      "2017-11-18 12:05:21,201 - begin [135] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:21,240 - score 0.832 time 0\n",
      "2017-11-18 12:05:21,243 - begin [136] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:21,279 - score 0.825 time 0\n",
      "2017-11-18 12:05:21,281 - begin [137] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:21,320 - score 0.825 time 0\n",
      "2017-11-18 12:05:21,324 - begin [138] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:21,361 - score 0.813 time 0\n",
      "2017-11-18 12:05:21,363 - begin [139] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:21,404 - score 0.813 time 0\n",
      "2017-11-18 12:05:21,407 - begin [140] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:21,451 - score 0.812 time 0\n",
      "2017-11-18 12:05:21,453 - begin [141] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:21,496 - score 0.800 time 0\n",
      "2017-11-18 12:05:21,498 - begin [142] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:21,537 - score 0.806 time 0\n",
      "2017-11-18 12:05:21,543 - begin [143] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 5, max_features = sqrt, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:21,577 - score 0.808 time 0\n",
      "2017-11-18 12:05:21,579 - begin [144] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:21,619 - score 0.811 time 0\n",
      "2017-11-18 12:05:21,623 - begin [145] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:21,668 - score 0.820 time 0\n",
      "2017-11-18 12:05:21,670 - begin [146] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:21,714 - score 0.825 time 0\n",
      "2017-11-18 12:05:21,715 - begin [147] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:21,756 - score 0.804 time 0\n",
      "2017-11-18 12:05:21,765 - begin [148] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:21,805 - score 0.808 time 0\n",
      "2017-11-18 12:05:21,808 - begin [149] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:21,844 - score 0.817 time 0\n",
      "2017-11-18 12:05:21,846 - begin [150] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:21,889 - score 0.804 time 0\n",
      "2017-11-18 12:05:21,892 - begin [151] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:21,935 - score 0.808 time 0\n",
      "2017-11-18 12:05:21,937 - begin [152] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = None, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:21,978 - score 0.819 time 0\n",
      "2017-11-18 12:05:21,980 - begin [153] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 100, subsample = 1.0}\n",
      "2017-11-18 12:05:22,027 - score 0.800 time 0\n",
      "2017-11-18 12:05:22,028 - begin [154] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 100, subsample = 0.8}\n",
      "2017-11-18 12:05:22,068 - score 0.810 time 0\n",
      "2017-11-18 12:05:22,070 - begin [155] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:22,114 - score 0.807 time 0\n",
      "2017-11-18 12:05:22,116 - begin [156] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 500, subsample = 1.0}\n",
      "2017-11-18 12:05:22,164 - score 0.792 time 0\n",
      "2017-11-18 12:05:22,165 - begin [157] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 500, subsample = 0.8}\n",
      "2017-11-18 12:05:22,218 - score 0.804 time 0\n",
      "2017-11-18 12:05:22,220 - begin [158] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 500, subsample = 0.6}\n",
      "2017-11-18 12:05:22,268 - score 0.808 time 0\n",
      "2017-11-18 12:05:22,270 - begin [159] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 1000, subsample = 1.0}\n",
      "2017-11-18 12:05:22,333 - score 0.789 time 0\n",
      "2017-11-18 12:05:22,336 - begin [160] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 1000, subsample = 0.8}\n",
      "2017-11-18 12:05:22,398 - score 0.805 time 0\n",
      "2017-11-18 12:05:22,400 - begin [161] for GradientBoostingClassifier {learning_rate = 0.1, max_depth = 8, max_features = sqrt, n_estimators = 1000, subsample = 0.6}\n",
      "2017-11-18 12:05:22,454 - score 0.811 time 0\n",
      "2017-11-18 12:05:22,461 - begin [162] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.001, kernel = linear}\n",
      "2017-11-18 12:05:22,511 - score 0.791 time 0\n",
      "2017-11-18 12:05:22,513 - begin [163] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.001, kernel = rbf}\n",
      "2017-11-18 12:05:22,551 - score 0.762 time 0\n",
      "2017-11-18 12:05:22,552 - begin [164] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.001, kernel = poly}\n",
      "2017-11-18 12:05:22,595 - score 0.470 time 0\n",
      "2017-11-18 12:05:22,596 - begin [165] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.01, kernel = linear}\n",
      "2017-11-18 12:05:22,642 - score 0.791 time 0\n",
      "2017-11-18 12:05:22,643 - begin [166] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.01, kernel = rbf}\n",
      "2017-11-18 12:05:22,685 - score 0.793 time 0\n",
      "2017-11-18 12:05:22,687 - begin [167] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.01, kernel = poly}\n",
      "2017-11-18 12:05:22,728 - score 0.742 time 0\n",
      "2017-11-18 12:05:22,730 - begin [168] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.1, kernel = linear}\n",
      "2017-11-18 12:05:22,771 - score 0.791 time 0\n",
      "2017-11-18 12:05:22,776 - begin [169] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.1, kernel = rbf}\n",
      "2017-11-18 12:05:22,818 - score 0.814 time 0\n",
      "2017-11-18 12:05:22,821 - begin [170] for SVC {C = 1.0, coef0 = 0.0, gamma = 0.1, kernel = poly}\n",
      "2017-11-18 12:05:22,863 - score 0.815 time 0\n",
      "2017-11-18 12:05:22,865 - begin [171] for SVC {C = 1.0, coef0 = 0.0, gamma = auto, kernel = linear}\n",
      "2017-11-18 12:05:22,912 - score 0.791 time 0\n",
      "2017-11-18 12:05:22,914 - begin [172] for SVC {C = 1.0, coef0 = 0.0, gamma = auto, kernel = rbf}\n",
      "2017-11-18 12:05:22,953 - score 0.825 time 0\n",
      "2017-11-18 12:05:22,960 - begin [173] for SVC {C = 1.0, coef0 = 0.0, gamma = auto, kernel = poly}\n",
      "2017-11-18 12:05:23,004 - score 0.823 time 0\n",
      "2017-11-18 12:05:23,010 - begin [174] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.001, kernel = linear}\n",
      "2017-11-18 12:05:23,067 - score 0.791 time 0\n",
      "2017-11-18 12:05:23,068 - begin [175] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.001, kernel = rbf}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-18 12:05:23,129 - score 0.762 time 0\n",
      "2017-11-18 12:05:23,131 - begin [176] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.001, kernel = poly}\n",
      "2017-11-18 12:05:23,185 - score 0.776 time 0\n",
      "2017-11-18 12:05:23,187 - begin [177] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.01, kernel = linear}\n",
      "2017-11-18 12:05:23,243 - score 0.791 time 0\n",
      "2017-11-18 12:05:23,244 - begin [178] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.01, kernel = rbf}\n",
      "2017-11-18 12:05:23,285 - score 0.793 time 0\n",
      "2017-11-18 12:05:23,288 - begin [179] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.01, kernel = poly}\n",
      "2017-11-18 12:05:23,331 - score 0.797 time 0\n",
      "2017-11-18 12:05:23,333 - begin [180] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.1, kernel = linear}\n",
      "2017-11-18 12:05:23,375 - score 0.791 time 0\n",
      "2017-11-18 12:05:23,379 - begin [181] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.1, kernel = rbf}\n",
      "2017-11-18 12:05:23,423 - score 0.814 time 0\n",
      "2017-11-18 12:05:23,426 - begin [182] for SVC {C = 1.0, coef0 = 1.0, gamma = 0.1, kernel = poly}\n",
      "2017-11-18 12:05:23,471 - score 0.819 time 0\n",
      "2017-11-18 12:05:23,477 - begin [183] for SVC {C = 1.0, coef0 = 1.0, gamma = auto, kernel = linear}\n",
      "2017-11-18 12:05:23,526 - score 0.791 time 0\n",
      "2017-11-18 12:05:23,527 - begin [184] for SVC {C = 1.0, coef0 = 1.0, gamma = auto, kernel = rbf}\n",
      "2017-11-18 12:05:23,574 - score 0.825 time 0\n",
      "2017-11-18 12:05:23,576 - begin [185] for SVC {C = 1.0, coef0 = 1.0, gamma = auto, kernel = poly}\n",
      "2017-11-18 12:05:23,620 - score 0.817 time 0\n",
      "2017-11-18 12:05:23,622 - begin [186] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.001, kernel = linear}\n",
      "2017-11-18 12:05:23,685 - score 0.791 time 0\n",
      "2017-11-18 12:05:23,687 - begin [187] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.001, kernel = rbf}\n",
      "2017-11-18 12:05:23,755 - score 0.762 time 0\n",
      "2017-11-18 12:05:23,760 - begin [188] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.001, kernel = poly}\n",
      "2017-11-18 12:05:23,821 - score 0.792 time 0\n",
      "2017-11-18 12:05:23,830 - begin [189] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.01, kernel = linear}\n",
      "2017-11-18 12:05:23,881 - score 0.791 time 0\n",
      "2017-11-18 12:05:23,885 - begin [190] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.01, kernel = rbf}\n",
      "2017-11-18 12:05:23,934 - score 0.793 time 0\n",
      "2017-11-18 12:05:23,936 - begin [191] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.01, kernel = poly}\n",
      "2017-11-18 12:05:23,978 - score 0.826 time 0\n",
      "2017-11-18 12:05:23,980 - begin [192] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.1, kernel = linear}\n",
      "2017-11-18 12:05:24,021 - score 0.791 time 0\n",
      "2017-11-18 12:05:24,027 - begin [193] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.1, kernel = rbf}\n",
      "2017-11-18 12:05:24,077 - score 0.814 time 0\n",
      "2017-11-18 12:05:24,079 - begin [194] for SVC {C = 1.0, coef0 = 10.0, gamma = 0.1, kernel = poly}\n",
      "2017-11-18 12:05:24,127 - score 0.816 time 0\n",
      "2017-11-18 12:05:24,128 - begin [195] for SVC {C = 1.0, coef0 = 10.0, gamma = auto, kernel = linear}\n",
      "2017-11-18 12:05:24,174 - score 0.791 time 0\n",
      "2017-11-18 12:05:24,176 - begin [196] for SVC {C = 1.0, coef0 = 10.0, gamma = auto, kernel = rbf}\n",
      "2017-11-18 12:05:24,224 - score 0.825 time 0\n",
      "2017-11-18 12:05:24,226 - begin [197] for SVC {C = 1.0, coef0 = 10.0, gamma = auto, kernel = poly}\n",
      "2017-11-18 12:05:24,274 - score 0.831 time 0\n",
      "2017-11-18 12:05:24,275 - begin [198] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.001, kernel = linear}\n",
      "2017-11-18 12:05:24,321 - score 0.827 time 0\n",
      "2017-11-18 12:05:24,325 - begin [199] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.001, kernel = rbf}\n",
      "2017-11-18 12:05:24,375 - score 0.787 time 0\n",
      "2017-11-18 12:05:24,378 - begin [200] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.001, kernel = poly}\n",
      "2017-11-18 12:05:24,426 - score 0.484 time 0\n",
      "2017-11-18 12:05:24,428 - begin [201] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.01, kernel = linear}\n",
      "2017-11-18 12:05:24,476 - score 0.827 time 0\n",
      "2017-11-18 12:05:24,478 - begin [202] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.01, kernel = rbf}\n",
      "2017-11-18 12:05:24,527 - score 0.824 time 0\n",
      "2017-11-18 12:05:24,529 - begin [203] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.01, kernel = poly}\n",
      "2017-11-18 12:05:24,577 - score 0.743 time 0\n",
      "2017-11-18 12:05:24,578 - begin [204] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.1, kernel = linear}\n",
      "2017-11-18 12:05:24,627 - score 0.827 time 0\n",
      "2017-11-18 12:05:24,629 - begin [205] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.1, kernel = rbf}\n",
      "2017-11-18 12:05:24,679 - score 0.809 time 0\n",
      "2017-11-18 12:05:24,681 - begin [206] for SVC {C = 10.0, coef0 = 0.0, gamma = 0.1, kernel = poly}\n",
      "2017-11-18 12:05:24,733 - score 0.813 time 0\n",
      "2017-11-18 12:05:24,735 - begin [207] for SVC {C = 10.0, coef0 = 0.0, gamma = auto, kernel = linear}\n",
      "2017-11-18 12:05:24,796 - score 0.827 time 0\n",
      "2017-11-18 12:05:24,799 - begin [208] for SVC {C = 10.0, coef0 = 0.0, gamma = auto, kernel = rbf}\n",
      "2017-11-18 12:05:24,864 - score 0.819 time 0\n",
      "2017-11-18 12:05:24,865 - begin [209] for SVC {C = 10.0, coef0 = 0.0, gamma = auto, kernel = poly}\n",
      "2017-11-18 12:05:24,931 - score 0.817 time 0\n",
      "2017-11-18 12:05:24,933 - begin [210] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.001, kernel = linear}\n",
      "2017-11-18 12:05:24,997 - score 0.827 time 0\n",
      "2017-11-18 12:05:24,999 - begin [211] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.001, kernel = rbf}\n",
      "2017-11-18 12:05:25,052 - score 0.787 time 0\n",
      "2017-11-18 12:05:25,054 - begin [212] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.001, kernel = poly}\n",
      "2017-11-18 12:05:25,107 - score 0.787 time 0\n",
      "2017-11-18 12:05:25,109 - begin [213] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.01, kernel = linear}\n",
      "2017-11-18 12:05:25,163 - score 0.827 time 0\n",
      "2017-11-18 12:05:25,168 - begin [214] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.01, kernel = rbf}\n",
      "2017-11-18 12:05:25,215 - score 0.824 time 0\n",
      "2017-11-18 12:05:25,221 - begin [215] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.01, kernel = poly}\n",
      "2017-11-18 12:05:25,272 - score 0.824 time 0\n",
      "2017-11-18 12:05:25,274 - begin [216] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.1, kernel = linear}\n",
      "2017-11-18 12:05:25,328 - score 0.827 time 0\n",
      "2017-11-18 12:05:25,329 - begin [217] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.1, kernel = rbf}\n",
      "2017-11-18 12:05:25,385 - score 0.809 time 0\n",
      "2017-11-18 12:05:25,391 - begin [218] for SVC {C = 10.0, coef0 = 1.0, gamma = 0.1, kernel = poly}\n",
      "2017-11-18 12:05:25,441 - score 0.811 time 0\n",
      "2017-11-18 12:05:25,446 - begin [219] for SVC {C = 10.0, coef0 = 1.0, gamma = auto, kernel = linear}\n",
      "2017-11-18 12:05:25,503 - score 0.827 time 0\n",
      "2017-11-18 12:05:25,505 - begin [220] for SVC {C = 10.0, coef0 = 1.0, gamma = auto, kernel = rbf}\n",
      "2017-11-18 12:05:25,548 - score 0.819 time 0\n",
      "2017-11-18 12:05:25,549 - begin [221] for SVC {C = 10.0, coef0 = 1.0, gamma = auto, kernel = poly}\n",
      "2017-11-18 12:05:25,601 - score 0.822 time 0\n",
      "2017-11-18 12:05:25,602 - begin [222] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.001, kernel = linear}\n",
      "2017-11-18 12:05:25,650 - score 0.827 time 0\n",
      "2017-11-18 12:05:25,653 - begin [223] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.001, kernel = rbf}\n",
      "2017-11-18 12:05:25,711 - score 0.787 time 0\n",
      "2017-11-18 12:05:25,713 - begin [224] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.001, kernel = poly}\n",
      "2017-11-18 12:05:25,767 - score 0.829 time 0\n",
      "2017-11-18 12:05:25,769 - begin [225] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.01, kernel = linear}\n",
      "2017-11-18 12:05:25,820 - score 0.827 time 0\n",
      "2017-11-18 12:05:25,826 - begin [226] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.01, kernel = rbf}\n",
      "2017-11-18 12:05:25,876 - score 0.824 time 0\n",
      "2017-11-18 12:05:25,877 - begin [227] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.01, kernel = poly}\n",
      "2017-11-18 12:05:25,932 - score 0.824 time 0\n",
      "2017-11-18 12:05:25,937 - begin [228] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.1, kernel = linear}\n",
      "2017-11-18 12:05:25,989 - score 0.827 time 0\n",
      "2017-11-18 12:05:25,992 - begin [229] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.1, kernel = rbf}\n",
      "2017-11-18 12:05:26,045 - score 0.809 time 0\n",
      "2017-11-18 12:05:26,048 - begin [230] for SVC {C = 10.0, coef0 = 10.0, gamma = 0.1, kernel = poly}\n",
      "2017-11-18 12:05:26,102 - score 0.801 time 0\n",
      "2017-11-18 12:05:26,107 - begin [231] for SVC {C = 10.0, coef0 = 10.0, gamma = auto, kernel = linear}\n",
      "2017-11-18 12:05:26,160 - score 0.827 time 0\n",
      "2017-11-18 12:05:26,161 - begin [232] for SVC {C = 10.0, coef0 = 10.0, gamma = auto, kernel = rbf}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-18 12:05:26,211 - score 0.819 time 0\n",
      "2017-11-18 12:05:26,213 - begin [233] for SVC {C = 10.0, coef0 = 10.0, gamma = auto, kernel = poly}\n",
      "2017-11-18 12:05:26,266 - score 0.814 time 0\n",
      "2017-11-18 12:05:26,267 - begin [234] for MLPClassifier {activation = relu, hidden_layer_sizes = (50,)}\n",
      "2017-11-18 12:05:26,272 - score 0.827 time 0\n",
      "2017-11-18 12:05:26,275 - begin [235] for MLPClassifier {activation = relu, hidden_layer_sizes = (100,)}\n",
      "2017-11-18 12:05:26,278 - score 0.833 time 0\n",
      "2017-11-18 12:05:26,281 - begin [236] for MLPClassifier {activation = relu, hidden_layer_sizes = (200,)}\n",
      "2017-11-18 12:05:26,286 - score 0.836 time 0\n",
      "2017-11-18 12:05:26,290 - begin [237] for MLPClassifier {activation = identity, hidden_layer_sizes = (50,)}\n",
      "2017-11-18 12:05:26,295 - score 0.816 time 0\n",
      "2017-11-18 12:05:26,297 - begin [238] for MLPClassifier {activation = identity, hidden_layer_sizes = (100,)}\n",
      "2017-11-18 12:05:26,301 - score 0.815 time 0\n",
      "2017-11-18 12:05:26,307 - begin [239] for MLPClassifier {activation = identity, hidden_layer_sizes = (200,)}\n",
      "2017-11-18 12:05:26,311 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,313 - begin [240] for MLPClassifier {activation = logistic, hidden_layer_sizes = (50,)}\n",
      "2017-11-18 12:05:26,317 - score 0.820 time 0\n",
      "2017-11-18 12:05:26,320 - begin [241] for MLPClassifier {activation = logistic, hidden_layer_sizes = (100,)}\n",
      "2017-11-18 12:05:26,324 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,326 - begin [242] for MLPClassifier {activation = logistic, hidden_layer_sizes = (200,)}\n",
      "2017-11-18 12:05:26,332 - score 0.819 time 0\n",
      "2017-11-18 12:05:26,336 - begin [243] for MLPClassifier {activation = tanh, hidden_layer_sizes = (50,)}\n",
      "2017-11-18 12:05:26,342 - score 0.824 time 0\n",
      "2017-11-18 12:05:26,345 - begin [244] for MLPClassifier {activation = tanh, hidden_layer_sizes = (100,)}\n",
      "2017-11-18 12:05:26,350 - score 0.819 time 0\n",
      "2017-11-18 12:05:26,353 - begin [245] for MLPClassifier {activation = tanh, hidden_layer_sizes = (200,)}\n",
      "2017-11-18 12:05:26,364 - score 0.817 time 0\n",
      "2017-11-18 12:05:26,369 - begin [246] for AdaBoostClassifier {learning_rate = 0.01, n_estimators = 50}\n",
      "2017-11-18 12:05:26,378 - score 0.779 time 0\n",
      "2017-11-18 12:05:26,380 - begin [247] for AdaBoostClassifier {learning_rate = 0.01, n_estimators = 100}\n",
      "2017-11-18 12:05:26,385 - score 0.787 time 0\n",
      "2017-11-18 12:05:26,388 - begin [248] for AdaBoostClassifier {learning_rate = 0.01, n_estimators = 300}\n",
      "2017-11-18 12:05:26,394 - score 0.801 time 0\n",
      "2017-11-18 12:05:26,400 - begin [249] for AdaBoostClassifier {learning_rate = 0.01, n_estimators = 500}\n",
      "2017-11-18 12:05:26,408 - score 0.817 time 0\n",
      "2017-11-18 12:05:26,410 - begin [250] for AdaBoostClassifier {learning_rate = 0.1, n_estimators = 50}\n",
      "2017-11-18 12:05:26,415 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,418 - begin [251] for AdaBoostClassifier {learning_rate = 0.1, n_estimators = 100}\n",
      "2017-11-18 12:05:26,427 - score 0.826 time 0\n",
      "2017-11-18 12:05:26,429 - begin [252] for AdaBoostClassifier {learning_rate = 0.1, n_estimators = 300}\n",
      "2017-11-18 12:05:26,436 - score 0.826 time 0\n",
      "2017-11-18 12:05:26,441 - begin [253] for AdaBoostClassifier {learning_rate = 0.1, n_estimators = 500}\n",
      "2017-11-18 12:05:26,447 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,450 - begin [254] for AdaBoostClassifier {learning_rate = 1.0, n_estimators = 50}\n",
      "2017-11-18 12:05:26,457 - score 0.810 time 0\n",
      "2017-11-18 12:05:26,459 - begin [255] for AdaBoostClassifier {learning_rate = 1.0, n_estimators = 100}\n",
      "2017-11-18 12:05:26,468 - score 0.809 time 0\n",
      "2017-11-18 12:05:26,474 - begin [256] for AdaBoostClassifier {learning_rate = 1.0, n_estimators = 300}\n",
      "2017-11-18 12:05:26,481 - score 0.817 time 0\n",
      "2017-11-18 12:05:26,483 - begin [257] for AdaBoostClassifier {learning_rate = 1.0, n_estimators = 500}\n",
      "2017-11-18 12:05:26,491 - score 0.822 time 0\n",
      "2017-11-18 12:05:26,494 - begin [258] for ExtraTreesClassifier {max_depth = 3, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:26,508 - score 0.807 time 0\n",
      "2017-11-18 12:05:26,510 - begin [259] for ExtraTreesClassifier {max_depth = 3, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:26,519 - score 0.805 time 0\n",
      "2017-11-18 12:05:26,521 - begin [260] for ExtraTreesClassifier {max_depth = 3, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:26,530 - score 0.801 time 0\n",
      "2017-11-18 12:05:26,537 - begin [261] for ExtraTreesClassifier {max_depth = 3, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:26,548 - score 0.807 time 0\n",
      "2017-11-18 12:05:26,550 - begin [262] for ExtraTreesClassifier {max_depth = 3, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:26,559 - score 0.791 time 0\n",
      "2017-11-18 12:05:26,561 - begin [263] for ExtraTreesClassifier {max_depth = 3, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:26,575 - score 0.792 time 0\n",
      "2017-11-18 12:05:26,577 - begin [264] for ExtraTreesClassifier {max_depth = 3, max_features = sqrt, n_estimators = 100}\n",
      "2017-11-18 12:05:26,587 - score 0.792 time 0\n",
      "2017-11-18 12:05:26,590 - begin [265] for ExtraTreesClassifier {max_depth = 3, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:26,601 - score 0.792 time 0\n",
      "2017-11-18 12:05:26,607 - begin [266] for ExtraTreesClassifier {max_depth = 3, max_features = log2, n_estimators = 10}\n",
      "2017-11-18 12:05:26,617 - score 0.791 time 0\n",
      "2017-11-18 12:05:26,620 - begin [267] for ExtraTreesClassifier {max_depth = 3, max_features = log2, n_estimators = 50}\n",
      "2017-11-18 12:05:26,631 - score 0.792 time 0\n",
      "2017-11-18 12:05:26,633 - begin [268] for ExtraTreesClassifier {max_depth = 3, max_features = log2, n_estimators = 100}\n",
      "2017-11-18 12:05:26,644 - score 0.792 time 0\n",
      "2017-11-18 12:05:26,648 - begin [269] for ExtraTreesClassifier {max_depth = 3, max_features = log2, n_estimators = 500}\n",
      "2017-11-18 12:05:26,657 - score 0.792 time 0\n",
      "2017-11-18 12:05:26,659 - begin [270] for ExtraTreesClassifier {max_depth = 5, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:26,672 - score 0.816 time 0\n",
      "2017-11-18 12:05:26,675 - begin [271] for ExtraTreesClassifier {max_depth = 5, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:26,683 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,687 - begin [272] for ExtraTreesClassifier {max_depth = 5, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:26,697 - score 0.814 time 0\n",
      "2017-11-18 12:05:26,701 - begin [273] for ExtraTreesClassifier {max_depth = 5, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:26,712 - score 0.814 time 0\n",
      "2017-11-18 12:05:26,715 - begin [274] for ExtraTreesClassifier {max_depth = 5, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:26,728 - score 0.820 time 0\n",
      "2017-11-18 12:05:26,730 - begin [275] for ExtraTreesClassifier {max_depth = 5, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:26,742 - score 0.822 time 0\n",
      "2017-11-18 12:05:26,745 - begin [276] for ExtraTreesClassifier {max_depth = 5, max_features = sqrt, n_estimators = 100}\n",
      "2017-11-18 12:05:26,759 - score 0.822 time 0\n",
      "2017-11-18 12:05:26,762 - begin [277] for ExtraTreesClassifier {max_depth = 5, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:26,774 - score 0.822 time 0\n",
      "2017-11-18 12:05:26,776 - begin [278] for ExtraTreesClassifier {max_depth = 5, max_features = log2, n_estimators = 10}\n",
      "2017-11-18 12:05:26,789 - score 0.820 time 0\n",
      "2017-11-18 12:05:26,793 - begin [279] for ExtraTreesClassifier {max_depth = 5, max_features = log2, n_estimators = 50}\n",
      "2017-11-18 12:05:26,805 - score 0.822 time 0\n",
      "2017-11-18 12:05:26,807 - begin [280] for ExtraTreesClassifier {max_depth = 5, max_features = log2, n_estimators = 100}\n",
      "2017-11-18 12:05:26,818 - score 0.822 time 0\n",
      "2017-11-18 12:05:26,824 - begin [281] for ExtraTreesClassifier {max_depth = 5, max_features = log2, n_estimators = 500}\n",
      "2017-11-18 12:05:26,837 - score 0.822 time 0\n",
      "2017-11-18 12:05:26,839 - begin [282] for ExtraTreesClassifier {max_depth = 8, max_features = None, n_estimators = 10}\n",
      "2017-11-18 12:05:26,853 - score 0.815 time 0\n",
      "2017-11-18 12:05:26,859 - begin [283] for ExtraTreesClassifier {max_depth = 8, max_features = None, n_estimators = 50}\n",
      "2017-11-18 12:05:26,873 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,875 - begin [284] for ExtraTreesClassifier {max_depth = 8, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:26,889 - score 0.822 time 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-11-18 12:05:26,891 - begin [285] for ExtraTreesClassifier {max_depth = 8, max_features = None, n_estimators = 500}\n",
      "2017-11-18 12:05:26,904 - score 0.821 time 0\n",
      "2017-11-18 12:05:26,906 - begin [286] for ExtraTreesClassifier {max_depth = 8, max_features = sqrt, n_estimators = 10}\n",
      "2017-11-18 12:05:26,917 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,922 - begin [287] for ExtraTreesClassifier {max_depth = 8, max_features = sqrt, n_estimators = 50}\n",
      "2017-11-18 12:05:26,940 - score 0.817 time 0\n",
      "2017-11-18 12:05:26,942 - begin [288] for ExtraTreesClassifier {max_depth = 8, max_features = sqrt, n_estimators = 100}\n",
      "2017-11-18 12:05:26,953 - score 0.817 time 0\n",
      "2017-11-18 12:05:26,955 - begin [289] for ExtraTreesClassifier {max_depth = 8, max_features = sqrt, n_estimators = 500}\n",
      "2017-11-18 12:05:26,971 - score 0.817 time 0\n",
      "2017-11-18 12:05:26,977 - begin [290] for ExtraTreesClassifier {max_depth = 8, max_features = log2, n_estimators = 10}\n",
      "2017-11-18 12:05:26,990 - score 0.818 time 0\n",
      "2017-11-18 12:05:26,995 - begin [291] for ExtraTreesClassifier {max_depth = 8, max_features = log2, n_estimators = 50}\n",
      "2017-11-18 12:05:27,012 - score 0.817 time 0\n",
      "2017-11-18 12:05:27,013 - begin [292] for ExtraTreesClassifier {max_depth = 8, max_features = log2, n_estimators = 100}\n",
      "2017-11-18 12:05:27,027 - score 0.817 time 0\n",
      "2017-11-18 12:05:27,029 - begin [293] for ExtraTreesClassifier {max_depth = 8, max_features = log2, n_estimators = 500}\n",
      "2017-11-18 12:05:27,046 - score 0.817 time 0\n",
      "2017-11-18 12:05:27,048 - level1 models\n",
      "2017-11-18 12:05:27,049 - [10] 0.828 0.017 LogisticRegression {C = 100, penalty = l1}\n",
      "2017-11-18 12:05:27,053 - [24] 0.824 0.009 RandomForestClassifier {max_depth = 3, max_features = None, n_estimators = 100}\n",
      "2017-11-18 12:05:27,058 - [110] 0.836 0.005 GradientBoostingClassifier {learning_rate = 0.1, max_depth = 3, max_features = None, n_estimators = 100, subsample = 0.6}\n",
      "2017-11-18 12:05:27,060 - [197] 0.831 0.020 SVC {C = 1.0, coef0 = 10.0, gamma = auto, kernel = poly}\n",
      "2017-11-18 12:05:27,065 - [235] 0.833 0.024 MLPClassifier {activation = relu, hidden_layer_sizes = (100,)}\n",
      "2017-11-18 12:05:27,066 - [251] 0.826 0.007 AdaBoostClassifier {learning_rate = 0.1, n_estimators = 100}\n",
      "2017-11-18 12:05:27,070 - [274] 0.820 0.020 ExtraTreesClassifier {max_depth = 5, max_features = sqrt, n_estimators = 10}\n",
      "d:\\app\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "TASK_NAME='stacking-best2-lr'\n",
    "logger,handlers=initLogging(TASK_NAME)\n",
    "\n",
    "aml=reload(aml)\n",
    "bc=aml.BinaryClassifier.auto(train_data,target,('lr','svc','rf','gbt','mlp','ada','ext'),last_result,logger=logger,cv=5,best_n=1)\n",
    "resetLogging(logger,handlers)\n",
    "bc.fit(train_data.values,target)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resetLogging(logger,handlers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\app\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "d:\\app\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "d:\\app\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "d:\\app\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "d:\\app\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gra 0.836 0.017\n",
      "SVC 0.825 0.024\n",
      "Bin 0.824 0.032\n",
      "Log 0.816 0.017\n",
      "Ran 0.811 0.035\n",
      "Ada 0.809 0.014\n",
      "Ext 0.806 0.043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier,LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "base_clfs=[\n",
    "    bc,\n",
    "#     bc.get_nth_model(0),\n",
    "#     bc.get_nth_model(1),\n",
    "#     SVC(probability=True,random_state=42,C=10.0,gamma=0.01,coef0=1.,degree=3,kernel='poly'),\n",
    "#     GradientBoostingClassifier(learning_rate=0.01,max_depth=5,max_features=None, n_estimators=100)\n",
    "    LogisticRegression(random_state=42),\n",
    "    SVC(probability=True,random_state=42),\n",
    "    RandomForestClassifier(random_state=42),\n",
    "    GradientBoostingClassifier(random_state=42),\n",
    "    AdaBoostClassifier(), \n",
    "    ExtraTreesClassifier(n_jobs=-1), \n",
    "#       XGBClassifier(),\n",
    "]\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "def trainModels(train_data, target):\n",
    "    results=[]\n",
    "    for clf in base_clfs:\n",
    "        scores = cross_val_score(clf,train_data,target,scoring='accuracy',cv=5)\n",
    "        results.append((clf.__class__.__name__[:3],np.mean(scores),np.std(scores)))\n",
    "    for r in sorted(results,key=lambda x:x[1],reverse=True):\n",
    "        print('%s %.3f %.3f'%r)\n",
    "#     labels=[c.__class__.__name__[:3] for c in base_clfs]\n",
    "#     X=np.arange(len(base_clfs))\n",
    "#     bar(X,scores,tick_label=labels,color='rgb')\n",
    "#     show()\n",
    "#     print(sorted(zip(labels,scores),key=lambda x:x[1],reverse=True))\n",
    "    \n",
    "trainModels(train_data.values,target.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
       "              max_features=None, max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=2,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=100,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bc.get_nth_model(0)\n",
    "# base_clfs[2]\n",
    "GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "TASK_NAME='gbt-01-3-None-500-08'\n",
    "clf=GradientBoostingClassifier(learning_rate=0.1,max_depth=3,max_features=None,n_estimators=500,subsample=0.8)\n",
    "# clf.fit(train_data,target)\n",
    "# bc.select(0)\n",
    "# clf=bc\n",
    "# clf=base_clfs[4]\n",
    "# clf=bc.get_nth_model(0)\n",
    "# clf=GradientBoostingClassifier(learning_rate=0.1,max_depth=3,max_features=None,n_estimators=500,subsample=0.8)\n",
    "# clf=GradientBoostingClassifier(n_estimators=500)\n",
    "clf.fit(train_data.values,target)\n",
    "# bc.fit_one(2,train_data.values,target)\n",
    "test_id=passenger_test[id_col]\n",
    "# clf=base_clfs[0]\n",
    "survived=clf.predict(test_data)\n",
    "test_Survived = pd.Series(survived, name=\"Survived\").astype(int)\n",
    "results = pd.concat([test_id,test_Survived],axis=1)\n",
    "results.to_csv(\"output/%s.csv\"%TASK_NAME,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
