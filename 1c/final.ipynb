{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Summary\n",
    "\n",
    "I work in a chinese big IT company. My recent projects are about network data analysis. I spend about 2 weeks on the competition.\n",
    "\n",
    "This notebook contains the all steps to produce the my final submissing, public and private LB scores are: 0.997700 and 0.999485.\n",
    "\n",
    "Anyway, I don't use the description or other complicate features because of time. So I think there is improvement definitely in future.\n",
    "\n",
    "I used common packages in python world, but xlearn is a exception. It is a factorization machine algorithm package which is useful in ctr prediction and recommendation system. The input data is libsvm format. The transformation process is time consuming. The link is here https://github.com/aksnzhy/xlearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats, integrate\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lightgbm import LGBMRegressor\n",
    "import xlearn as xl\n",
    "\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set(color_codes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train=pd.read_csv('raw_data/sales_train.csv.gz')\n",
    "sales_test=pd.read_csv('raw_data/test.csv.gz')\n",
    "items=pd.read_csv('raw_data/items.csv')\n",
    "item_categories=pd.read_csv('raw_data/item_categories.csv')\n",
    "\n",
    "# rename the column for simplicity\n",
    "sales_train.rename(columns={'date_block_num':'block'},inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train data summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "nearly 3 million records without missing values\n",
    "\n",
    "shop_id and item_id are categorical variables\n",
    "\n",
    "item_price, item_cnt_day are numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### groupby shop, item, block to build target dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_month=sales_train.groupby(['shop_id','item_id','block'],as_index=False).agg({'item_cnt_day':'sum','item_price':'mean'}).rename(columns={'item_cnt_day':'item_cnt_month'})\n",
    "sales_month.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explore item price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "items_avg_price=sales_month.groupby('item_id')['item_price'].mean()\n",
    "f, ax = plt.subplots(figsize=(15, 6));\n",
    "sns.distplot(items_avg_price,kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the prices above 20000 are scarce. so clip the high values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u,l=np.percentile(items_avg_price,[0,95])\n",
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.distplot(np.clip(items_avg_price,u,l),kde=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most prices are range from 0 to 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='item_price',y='item_cnt_month',data=sales_month[sales_month.item_price<2000]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "item_price and item_month_cnt are not strong corralated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explore month block / time sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.barplot(x='block',y='item_cnt_month',data=sales_month.groupby('block',as_index=False)['item_cnt_month'].sum());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it seems the total sales decreese every month.\n",
    "\n",
    "christmas months are spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.barplot(x='block',y='item_cnt_month',data=sales_month.groupby(['block'],as_index=False)['item_cnt_month'].count());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the month shop&item sale pair is about 40k average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explore shops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shop_sales=sales_month.groupby('shop_id',as_index=False)[['item_cnt_month']].sum().sort_values('item_cnt_month',ascending=False)\n",
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.barplot(x='shop_id',y='item_cnt_month',data=shop_sales,order=shop_sales.shop_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "shop 31 is the largest shop\n",
    "\n",
    "near 2/3 shop sales cnt are close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### explore items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sales=sales_month.groupby('item_id',as_index=False)[['item_cnt_month']].sum().sort_values('item_cnt_month',ascending=False)\n",
    "f, ax = plt.subplots(figsize=(15, 6))\n",
    "sns.barplot(x='item_id',y='item_cnt_month',data=item_sales[:20],order=item_sales[:20].item_id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sales.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "it shows the avg item sale cnt is 167. below 75% is 124. some item like 20949 is very large which exceeds 170000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### explore test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the test shop&item pairs is 210k, which is bigger too much corresponding 40k. \n",
    "\n",
    "so not every shop item pair will have sale cnt, e.g. some pairs' cnt should be zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data=sales_test.groupby(['shop_id'],as_index=False)[['item_id']].count(),x='shop_id',y='item_id');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the plot shows every shop has about 5000 items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales_test.item_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gotcha! the total sale items is 5100. so the test set is produced by cross-products of shops and items in test month."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.Feature engineering and Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's build train and validation set\n",
    "\n",
    "train and validation data should be cross-producted like test set.\n",
    "\n",
    "I hold the last month in train data as validation data.\n",
    "\n",
    "As features, I use previous month sales, item category, shop month sales, item month sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def downcast_dtypes(df):\n",
    "    '''\n",
    "        Changes column types in the dataframe: \n",
    "                \n",
    "                `float64` type to `float32`\n",
    "                `int64`   type to `int32`\n",
    "        use to compact dataset\n",
    "    '''\n",
    "    \n",
    "    # Select columns to downcast\n",
    "    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n",
    "    int_cols =   [c for c in df if df[c].dtype == \"int64\"]\n",
    "    \n",
    "    # Downcast\n",
    "    df[float_cols] = df[float_cols].astype(np.float32)\n",
    "    df[int_cols]   = df[int_cols].astype(np.int32)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def clip_y(y):\n",
    "    '''\n",
    "    predicted cnt value should be in [0,20]\n",
    "    '''\n",
    "    return np.clip(y,0,20)\n",
    "\n",
    "def score(y_t,y_p):\n",
    "    return mean_squared_error(clip_y(y_t),clip_y(y_p))**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "label_col='item_cnt_month'\n",
    "\n",
    "# the last train block is validate block\n",
    "val_block=np.max(sales_train.block)\n",
    "\n",
    "# cross product shops and items\n",
    "sales_cross=[]\n",
    "index_cols=['shop_id','item_id']\n",
    "for d in sorted(sales_train.block.unique()):\n",
    "    m_sales=sales_train[sales_train.block==d]\n",
    "    m_si=pd.MultiIndex.from_product([m_sales['shop_id'].unique(),m_sales['item_id'].unique()],\n",
    "                                          names=index_cols).to_frame(index=False)\n",
    "    \n",
    "    m_si=m_si.merge(m_sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':'sum'}),\n",
    "                    on=index_cols,how='left').rename(columns={'item_cnt_day':label_col})\n",
    "    m_si['block']=d\n",
    "    sales_cross.append(m_si)\n",
    "\n",
    "\n",
    "sales_test['block']=val_block+1\n",
    "\n",
    "sales_cross.append(sales_test.drop('ID',axis=1))\n",
    "\n",
    "# don't use 2013 data\n",
    "sales=pd.concat(sales_cross)[lambda df:df.block>12]\n",
    "sales.fillna(0,inplace=True)\n",
    "\n",
    "sales_p=sales.copy()\n",
    "\n",
    "# joined with shop month sales\n",
    "shop_month_sales=sales_train.groupby(['shop_id','block'],as_index=False).agg({\n",
    "    'item_id':'nunique',\n",
    "    'item_cnt_day':'sum'\n",
    "}).rename(columns={\n",
    "    'item_id':'shop_item_id_cnt_month',\n",
    "    'item_cnt_day':'shop_item_cnt_month'\n",
    "})\n",
    "shop_month_sales['shop_item_avg_cnt_month']=shop_month_sales['shop_item_cnt_month']/shop_month_sales['shop_item_id_cnt_month']\n",
    "shop_month_sales.drop(['shop_item_cnt_month','shop_item_id_cnt_month'],axis=1,inplace=True)\n",
    "sales_p=sales_p.merge(shop_month_sales,on=['shop_id','block'],how='left')\n",
    "shop_month_cols=shop_month_sales.columns.difference(['shop_id','block'])\n",
    "\n",
    "# joined with item month sales\n",
    "item_month_sales=sales_train.groupby(['item_id','block'],as_index=False).agg({\n",
    "    'shop_id':'nunique',\n",
    "    'item_cnt_day':'sum',\n",
    "    'item_price':lambda s:np.mean(s)\n",
    "}).rename(columns={\n",
    "    'shop_id':'item_shop_cnt_month',\n",
    "    'item_cnt_day':'item_shop_sale_month'\n",
    "})\n",
    "item_month_sales['item_shop_avg_cnt_month']=item_month_sales['item_shop_sale_month']/item_month_sales['item_shop_cnt_month']\n",
    "item_month_sales.drop(['item_shop_cnt_month','item_shop_sale_month'],axis=1,inplace=True)\n",
    "sales_p=sales_p.merge(item_month_sales,on=['item_id','block'],how='left')\n",
    "item_month_cols=item_month_sales.columns.difference(['item_id','block'])\n",
    "\n",
    "# build previous month sales\n",
    "index_cols=['shop_id','item_id','block']\n",
    "merge_cols=sales_p.columns.difference(index_cols)\n",
    "for i in [1,2,3,]:\n",
    "    prev=sales_p[merge_cols.union(index_cols)].copy()\n",
    "    prev.block+=i\n",
    "    prev.rename(columns=lambda c:'prev_%s_%d'%(c,i) if c in merge_cols else c,inplace=True)\n",
    "    sales_p=sales_p.merge(prev,on=index_cols,how='left')\n",
    "\n",
    "sales_p=sales_p.merge(items.drop(['item_name'],axis=1),how='left',on='item_id')\n",
    "\n",
    "# joined with item category\n",
    "item_cats=items.groupby(['item_category_id'],as_index=False).agg({\n",
    "    'item_id':'count'\n",
    "}).rename(columns={'item_id':'item_cat_items_cnt'})\n",
    "sales_p=sales_p.merge(item_cats,on='item_category_id',how='left')\n",
    "\n",
    "sales_p=downcast_dtypes(sales_p)\n",
    "sales_p.fillna(0,inplace=True)\n",
    "\n",
    "# drop current month data\n",
    "drop_cols=set([label_col])\n",
    "ext_cols=[shop_month_cols,item_month_cols]\n",
    "for ec in ext_cols:\n",
    "    drop_cols = drop_cols|set(ec)\n",
    "\n",
    "# split train, validation, test data\n",
    "val_block=np.max(sales_p.block)-1\n",
    "X_train,X_val,X_test,y_train,y_val,y_test=sales_p.drop(drop_cols,axis=1)[lambda df:df.block<val_block],\\\n",
    "sales_p.drop(drop_cols,axis=1)[lambda df:df.block==val_block],\\\n",
    "sales_p.drop(drop_cols,axis=1)[lambda df:df.block==val_block+1],\\\n",
    "sales_p[sales_p.block<val_block][label_col],\\\n",
    "sales_p[sales_p.block==val_block][label_col],\\\n",
    "sales_p[sales_p.block==val_block+1][label_col],\\\n",
    "\n",
    "cat_cols=['shop_id','item_id','item_category_id']\n",
    "\n",
    "# after feature engineering and data split, use base predict values to test validation data\n",
    "print('base predict score %.4f\\n'%mean_squared_error(clip_y(y_val),np.ones(y_val.shape[0])*0.5)**0.5)\n",
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.train models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use two models. lightgbm is tree model. xlearn is ffm model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 train by gbdt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightgbm has early_stopping mechanism by providing validation data\n",
    "\n",
    "I get validation score 0.9632"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "lgb1=LGBMRegressor(n_jobs=8)\n",
    "lgb1.fit(X_train,y_train,\n",
    "        eval_set=(X_val,y_val),\n",
    "        early_stopping_rounds=1,\n",
    "        eval_metric=lambda y_t,y_p:('error',score(y_t,y_p),False),\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 train by gbdt with mean encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lightgbm has target encoding mechanism for categorical variables\n",
    "\n",
    "I remove the shop and item summary features, get validation score 0.9892"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "drop_cols=[c for c in X_train.columns if c.startswith('prev_') and not c.startswith('prev_item_cnt_month')]\n",
    "X_train_lgb2=X_train.copy()\n",
    "X_val_lgb2=X_val.copy()\n",
    "X_test_lgb2=X_test.copy()\n",
    "for x in X_train_lgb2,X_val_lgb2,X_test_lgb2:\n",
    "    x.drop(drop_cols,axis=1,inplace=True)\n",
    "\n",
    "lgb2=LGBMRegressor(n_jobs=8)\n",
    "lgb2.fit(X_train_lgb2,y_train,\n",
    "        eval_set=(X_val_lgb2,y_val),\n",
    "        early_stopping_rounds=1,\n",
    "        eval_metric=lambda y_t,y_p:('error',score(y_t,y_p),False),\n",
    "        categorical_feature=cat_cols,\n",
    "       )\n",
    "\n",
    "# y_pred_val_lgb2=clip_y(lgb2.predict(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 train by ffm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ffm is useful in most categorical datasets. but transform dataset into libsvm format is very time consuming.\n",
    "\n",
    "I get validation score 1.0032"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "def df_to_ffm(x,y,file,cat_cols):\n",
    "    x.index=np.arange(x.shape[0])\n",
    "    y.index=np.arange(x.shape[0])\n",
    "    cat_idx=set([-1])\n",
    "    for i in range(x.shape[1]):\n",
    "        if x.columns[i] in cat_cols:\n",
    "            cat_idx.add(i)\n",
    "            \n",
    "    with open(file,'w') as f:\n",
    "        for i in range(x.shape[0]):\n",
    "            s=str(y[i])\n",
    "            \n",
    "            for j in range(x.shape[1]):\n",
    "                if j in cat_idx:\n",
    "                    s += ' %d:%d:1' % (j,x.iat[i,j])\n",
    "                else:\n",
    "                    if x.iat[i,j]<0:\n",
    "                        s += ' %d:%d:1' % (j,0)\n",
    "                    else:\n",
    "                        s += ' %d:%d:1' % (j,x.iat[i,j])\n",
    "                    \n",
    "            f.write(s+'\\n')\n",
    "\n",
    "df_to_ffm(X_train,y_train,'ffm_data/train.ffm',[])\n",
    "df_to_ffm(X_val,y_val,'ffm_data/val.ffm',[])\n",
    "df_to_ffm(X_test,y_test,'ffm_data/test.ffm',[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "ffm_model = xl.create_ffm()\n",
    "ffm_model.setTrain(\"ffm_data/train.ffm\")  # Training data\n",
    "ffm_model.setTest('ffm_data/test.ffm')\n",
    "ffm_model.setValidate(\"ffm_data/val.ffm\")  # Validation data\n",
    "\n",
    "param = {'task':'reg','epoch':15,'lr':1e-2,'k':4,'lambda':2e-3}\n",
    "\n",
    "ffm_model.fit(param, 'ffm_data/model.out')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 ensemble\n",
    "\n",
    "I use the three models' validation results to train a ensemble regressor. \n",
    "\n",
    "I choose a gbdt to stacking the results to get LB 0.997700 and 0.999485."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_val_lgb1=clip_y(lgb1.predict(X_val))\n",
    "y_pred_test_lgb1=clip_y(lgb1.predict(X_test))\n",
    "\n",
    "y_pred_val_lgb2=clip_y(lgb2.predict(X_val_lgb2))\n",
    "y_pred_test_lgb2=clip_y(lgb2.predict(X_test_lgb2))\n",
    "\n",
    "ffm_model.setTest('ffm_data/val.ffm')\n",
    "ffm_model.predict('ffm_data/model.out','ensemble/ffm_val.csv')\n",
    "y_pred_val_ffm=clip_y(pd.read_csv('ensemble/ffm_val.csv',header=None))\n",
    "\n",
    "ffm_model.setTest('ffm_data/test.ffm')\n",
    "ffm_model.predict('ffm_data/model.out','ensemble/ffm_tst.csv')\n",
    "y_pred_test_ffm=clip_y(pd.read_csv('ensemble/ffm_tst.csv',header=None))\n",
    "\n",
    "stacking_reg=LGBMRegressor()\n",
    "stacking_reg.fit(np.c_[y_pred_val_lgb1,y_pred_val_lgb2,y_pred_val_ffm],y_val)\n",
    "sales_test[label_col]=clip_y(stacking_reg.predict(np.c_[y_pred_test_lgb1,y_pred_test_lgb2,y_pred_test_ffm]))\n",
    "\n",
    "sales_test[['ID',label_col]].to_csv('output/final.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
