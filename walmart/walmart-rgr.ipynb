{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import arima\n",
    "import datetime\n",
    "from sklearn.preprocessing import Imputer\n",
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pipeline import *\n",
    "from onehot import *\n",
    "from util import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('raw_data/features.csv')\n",
    "features.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[np.logical_not(np.isnan(features.MarkDown3))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stores = pd.read_csv('raw_data/stores.csv')\n",
    "stores.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('raw_data/train.csv')\n",
    "test_data = pd.read_csv('raw_data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "use lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# full_train=train_data.merge(stores,left_on='Store',right_on='Store').merge(features,left_on=('Store','Date'),right_on=('Store','Date'))\n",
    "# full_train['Week_In_Year']=full_train.Date.astype(np.datetime64).apply(lambda d: datetime.date.isocalendar(d)[1])\n",
    "# full_train.Date=full_train.Date.astype(np.datetime64)\n",
    "# full_train['IsHoliday']=full_train.IsHoliday_x.astype('int')\n",
    "# # full_train['Store']=full_train.Store.astype('U')\n",
    "# # full_train['Dept']=full_train.Dept.astype('U')\n",
    "# full_train['Week_In_Year']=full_train.Week_In_Year.astype('U')\n",
    "# full_train.drop(['IsHoliday_x','IsHoliday_y'],axis=1,inplace=True)\n",
    "# depts1=full_train[full_train.Dept==1][['Store','Dept','Type','Date','Weekly_Sales','Size','Temperature','Fuel_Price','CPI','Unemployment','IsHoliday','Week_In_Year']]\n",
    "# depts1_isHoliday=depts1.IsHoliday\n",
    "# depts1_train=depts1[depts1.Date<np.datetime64('2012-02-05')]\n",
    "# depts1_test=depts1[depts1.Date>=np.datetime64('2012-02-05')]\n",
    "# for df in (depts1_train,depts1_test):\n",
    "#     df.index=np.arange(len(df))\n",
    "    \n",
    "\n",
    "\n",
    "# full_pipeline=DataFramePipeline([\n",
    "# #     FeaturePipeline('Store','',Pipeline([('onehot',LabelBinarizerEx(['Store']))])),\n",
    "# #     FeaturePipeline('Dept','',Pipeline([('onehot',LabelBinarizerEx(['Dept']))])),\n",
    "# #     FeaturePipeline('Type','',Pipeline([('onehot',LabelBinarizerEx(['Type']))])),\n",
    "# #     FeaturePipeline('Size','Size',Pipeline([('scale',StandardScaler())])),\n",
    "# #     FeaturePipeline('Temperature','Temperature',Pipeline([('scale',StandardScaler())])),\n",
    "# #     FeaturePipeline('Fuel_Price','Fuel_Price',Pipeline([('scale',StandardScaler())])),\n",
    "# #     FeaturePipeline('CPI','CPI',Pipeline([('scale',StandardScaler())])),\n",
    "# #     FeaturePipeline('Unemployment','Unemployment',Pipeline([('scale',StandardScaler())])),\n",
    "#     FeaturePipeline('Week_In_Year','',Pipeline([('onehot',LabelBinarizerEx(['Week_In_Year']))])),\n",
    "# ]\n",
    "# )\n",
    "\n",
    "# depts1p_train=full_pipeline.fit_transform(depts1_train)\n",
    "# depts1p_train.drop(['Store','Dept','Type','Week_In_Year','IsHoliday','Date'],axis=1,inplace=True)\n",
    "# depts1p_target=depts1p_train.Weekly_Sales\n",
    "# depts1p_train=depts1p_train.drop('Weekly_Sales',axis=1)\n",
    "# depts1p_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# lr=LinearRegression()\n",
    "# gbt=GradientBoostingRegressor()\n",
    "# lr.fit(depts1p_train,depts1p_target)\n",
    "# sorted(zip(depts1_train.columns,lr.coef_),key=lambda x:x[1],reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# depts1p_test=full_pipeline.transform(depts1_test)\n",
    "# depts1p_test_isHoliday=depts1p_test.IsHoliday\n",
    "# depts1p_test.drop(['Store','Dept','Type','Week_In_Year','IsHoliday','Date'],axis=1,inplace=True)\n",
    "# depts1p_sales=depts1p_test.Weekly_Sales\n",
    "# depts1p_test=depts1p_test.drop('Weekly_Sales',axis=1)\n",
    "# depts1p_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score(predict_data,validate_data,isHoliday):\n",
    "    weights = np.array([5 if holiday else 1 for holiday in isHoliday])\n",
    "    return np.sum(np.abs((predict_data-validate_data)*weights)) / np.sum(weights)\n",
    "\n",
    "# score(lr.predict(depts1p_test),depts1p_sales,depts1p_test_isHoliday)\n",
    "# len(lr.predict(depts1p_test)),len(depts1p_sales),len(depts1p_test_isHoliday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gbt=GradientBoostingRegressor(n_estimators=100)\n",
    "# gbt.fit(depts1p_train,depts1p_target)\n",
    "# score(gbt.predict(depts1p_test),depts1p_sales,depts1p_test_isHoliday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sorted(zip(depts1p_train.columns,gbt.feature_importances_),key=lambda x:x[1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression,SGDClassifier,LinearRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC,LinearSVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier,XGBRegressor\n",
    "\n",
    "base_clfs=[\n",
    "      LinearRegression(n_jobs=-1),\n",
    "    \n",
    "      RandomForestRegressor(n_jobs=-1),\n",
    "      GradientBoostingRegressor(),\n",
    "    XGBRegressor(),\n",
    "#       AdaBoostClassifier(), \n",
    "#       ExtraTreesClassifier(n_jobs=-1)\n",
    "]\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import *\n",
    "\n",
    "def trainModels(train_data, target):\n",
    "    scores=[cross_val_score(clf,train_data,target,scoring='neg_mean_absolute_error',cv=5,verbose=1).mean() for clf in base_clfs]\n",
    "\n",
    "    labels=[c.__class__.__name__[:3] for c in base_clfs]\n",
    "#     X=np.arange(len(base_clfs))\n",
    "#     bar(X,scores,tick_label=labels,color='rgb')\n",
    "#     ylim(0.5,1.0)\n",
    "#     show()\n",
    "    print(sorted(zip(labels,scores),key=lambda x:x[1],reverse=True))\n",
    "    \n",
    "# trainModels(depts1p_train, depts1p_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# param_grid_set=[\n",
    "# #                 {'C':[0.01,0.1,0.5,1.]},\n",
    "# #                 {'C':[1.,10.,],'kernel':['rbf','poly'],'gamma':[0.01,0.1,1.],'coef0':[1.,10.,]},\n",
    "# #                 {'n_estimators':[50,100,200,300],'max_depth':[5,10,15]},\n",
    "# #                 {'learning_rate':[0.01,0.1,1.0],'n_estimators':[100,200,300],'max_depth':[3,5,8]},\n",
    "#     {'learning_rate':[0.1],'n_estimators':[1500,2000,3000],'max_depth':[5]},\n",
    "# #                 {'learning_rate':[0.01,0.1,1.0],'n_estimators':[100,200,300]},\n",
    "# #                 {'n_estimators':[50,100,200,300],'max_depth':[5,10,15]},\n",
    "# #                 {'learning_rate':[0.01,0.1,1.0],'n_estimators':[100,200,300],'max_depth':[5,10,15],'gamma':[0.01,0.1,0.5]},\n",
    "#                ]\n",
    "\n",
    "# base_clfs=[GradientBoostingRegressor()]\n",
    "# def tuneModels(train_data,target):\n",
    "#     results=[]\n",
    "#     for i in range(len(base_clfs)):\n",
    "#         gs=GridSearchCV(estimator=base_clfs[i],param_grid=param_grid_set[i],scoring=score,verbose=1,cv=5)\n",
    "#         gs.fit(train_data,target)\n",
    "#         results.append((gs.best_estimator_,gs.best_score_))\n",
    "#     print(sorted(results,key=lambda x:x[1],reverse=True))\n",
    "#     return results\n",
    "\n",
    "# tuneModels(depts1_train, depts1_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('raw_data/train.csv')\n",
    "test_data = pd.read_csv('raw_data/test.csv')\n",
    "\n",
    "all_df=[train_data,test_data]\n",
    "\n",
    "for i in range(len(all_df)):\n",
    "    df=all_df[i]\n",
    "    all_df[i]=df.merge(stores,left_on='Store',right_on='Store').merge(features,left_on=('Store','Date'),right_on=('Store','Date'))\n",
    "    df=all_df[i]\n",
    "    df.Date=df.Date.astype(np.datetime64)\n",
    "    df['Week_In_Year']=df.Date.apply(lambda d: datetime.date.isocalendar(d)[1])\n",
    "    df['Year']=df.Date.apply(lambda d:d.year)\n",
    "    \n",
    "    df['IsHoliday']=df.IsHoliday_x.astype('int')\n",
    "    df['Store']=df.Store.astype('U')\n",
    "    df['Week_In_Year']=df.Week_In_Year.astype('U')\n",
    "    df.drop(['IsHoliday_x','IsHoliday_y'],axis=1,inplace=True)\n",
    "    \n",
    "train_data=all_df[0]\n",
    "test_data=all_df[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# t=time.time()\n",
    "\n",
    "# models={}\n",
    "# depts=sorted(test_data.Dept.unique())\n",
    "# for d in depts:\n",
    "#     depts_train=train_data[train_data.Dept==d][['Store','Type','Weekly_Sales','Size',\n",
    "#                                                 'Temperature','Fuel_Price','CPI','Unemployment','Week_In_Year']]\n",
    "#     depts_train.index=np.arange(len(depts_train))\n",
    "#     sales_train=depts_train.Weekly_Sales\n",
    "    \n",
    "#     pipeline=DataFramePipeline([\n",
    "#         FeaturePipeline('Store','',Pipeline([('onehot',LabelBinarizerEx(['Store']))])),\n",
    "#         FeaturePipeline('Type','',Pipeline([('onehot',LabelBinarizerEx(['Type']))])),\n",
    "#         FeaturePipeline('Temperature','Temperature',Pipeline([('impute',Imputer(strategy='median'))])),\n",
    "#         FeaturePipeline('Fuel_Price','Fuel_Price',Pipeline([('impute',Imputer(strategy='median'))])),\n",
    "#         FeaturePipeline('CPI','CPI',Pipeline([('impute',Imputer(strategy='median'))])),\n",
    "#         FeaturePipeline('Unemployment','Unemployment',Pipeline([('impute',Imputer(strategy='median'))])),\n",
    "#         FeaturePipeline('Week_In_Year','',Pipeline([('onehot',LabelBinarizerEx(['Week_In_Year']))])),\n",
    "#     ]\n",
    "#     )\n",
    "#     depts_p=pipeline.fit_transform(depts_train)\n",
    "#     depts_p.drop(['Weekly_Sales','Store','Type','Week_In_Year'],axis=1,inplace=True)\n",
    "    \n",
    "#     gbt=GradientBoostingRegressor(n_estimators=1000)\n",
    "#     gbt.fit(depts_p,sales_train)\n",
    "#     models[d]=(pipeline, gbt)\n",
    "    \n",
    "# preds=[]\n",
    "# store_depts=test_data.groupby(['Store','Dept']).count().index\n",
    "# for sd in store_depts:\n",
    "#     raw_test=test_data[np.logical_and(test_data.Store==sd[0],test_data.Dept==sd[1])]\n",
    "#     depts_test=raw_test[['Store','Type','Size','Temperature','Fuel_Price','CPI','Unemployment','Week_In_Year']]\n",
    "#     depts_test.index=np.arange(len(depts_test))\n",
    "\n",
    "#     pipeline=models[sd[1]][0]\n",
    "#     depts_p=pipeline.transform(depts_test)\n",
    "#     depts_p.drop(['Store','Type','Week_In_Year'],axis=1,inplace=True)\n",
    "#     sales_pred=models[sd[1]][1].predict(depts_p)\n",
    "#     preds.append(pd.DataFrame({'Id':str(sd[0])+'_'+str(sd[1])+'_'+raw_test.Date.astype('U'),'Weekly_Sales':sales_pred.astype('int')}))\n",
    "    \n",
    "# result=pd.concat(preds)\n",
    "# result.to_csv('output/regression/result.csv',index=False,header=True)\n",
    "\n",
    "# 'total time %ds' % int(time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score 3235, rank 236\n",
    "\n",
    "features excluding isHoliday, markdowns\n",
    "\n",
    "score 3047\n",
    "\n",
    "features add year, gbdt 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot_pipeline=DataFramePipeline([\n",
    "    FeaturePipeline('Store','',Pipeline([('onehot',LabelBinarizerEx(['Store']))])),\n",
    "    FeaturePipeline('Type','',Pipeline([('onehot',LabelBinarizerEx(['Type']))])),\n",
    "    FeaturePipeline('Week_In_Year','',Pipeline([('onehot',LabelBinarizerEx(['Week_In_Year']))])),\n",
    "]\n",
    ")\n",
    "train_p = onehot_pipeline.fit_transform(train_data)\n",
    "train_p.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "md_cols=['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']\n",
    "hasMarkDown=np.array([False]*len(train_p))\n",
    "for c in md_cols:\n",
    "    hasMarkDown=np.logical_or(np.logical_not(np.isnan(train_p[c])),hasMarkDown)\n",
    "    \n",
    "train_md=train_p[hasMarkDown]\n",
    "train_md=train_md.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "t=time.time()\n",
    "\n",
    "models_md={}\n",
    "models={}\n",
    "depts=sorted(test_data.Dept.unique())\n",
    "for d in depts:\n",
    "    print('dept %d'%d)\n",
    "    sales_train=train_p[train_p.Dept==d].Weekly_Sales\n",
    "    depts_train=train_p[train_p.Dept==d].drop(['Store','Type','Weekly_Sales','Week_In_Year','Date','IsHoliday']+md_cols,axis=1)\n",
    "    depts_train.index=np.arange(len(depts_train))\n",
    "    \n",
    "    gbt=GradientBoostingRegressor(n_estimators=1000)\n",
    "    gbt.fit(depts_train,sales_train)\n",
    "    models[d]=gbt\n",
    "    \n",
    "#     sales_train=train_md[train_md.Dept==d].Weekly_Sales\n",
    "#     depts_train=train_md[train_md.Dept==d].drop(['Store','Type','Weekly_Sales','Week_In_Year','Date','IsHoliday'],axis=1)\n",
    "#     depts_train.index=np.arange(len(depts_train))\n",
    "\n",
    "#     gbt=RandomForestRegressor(n_estimators=300,max_depth=10)\n",
    "#     gbt.fit(depts_train,sales_train)\n",
    "#     models_md[d]=gbt\n",
    "    \n",
    "\n",
    "'total time %ds' % int(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=time.time()\n",
    "\n",
    "test_p=onehot_pipeline.transform(test_data)\n",
    "test_pipelines={}\n",
    "stores=test_p.Store.unique()\n",
    "for st in stores:\n",
    "    pipeline=DataFramePipeline([\n",
    "        FeaturePipeline('CPI','CPI',Pipeline([('impute',Imputer(strategy='median'))])),\n",
    "        FeaturePipeline('Unemployment','Unemployment',Pipeline([('impute',Imputer(strategy='median'))])),\n",
    "    ]\n",
    "    )\n",
    "    pipeline.fit(test_p[test_p.Store==st])\n",
    "    test_pipelines[st]=pipeline\n",
    "    \n",
    "preds=[]\n",
    "store_depts=test_p.groupby(['Store','Dept']).count().index\n",
    "md_pred=0\n",
    "\n",
    "for sd in store_depts:\n",
    "    depts_test=test_p[np.logical_and(test_p.Store==sd[0],test_p.Dept==sd[1])]\n",
    "    depts_test.index=np.arange(len(depts_test))\n",
    "    \n",
    "    \n",
    "    depts_p=test_pipelines[sd[0]].transform(depts_test)\n",
    "    \n",
    "#     hasMarkDown=np.array([False]*len(depts_p))\n",
    "#     for c in md_cols:\n",
    "#         hasMarkDown=np.logical_or(np.logical_not(np.isnan(depts_p[c])),hasMarkDown)\n",
    "\n",
    "#     test_md=depts_p[hasMarkDown]\n",
    "#     if len(test_md)>0:\n",
    "#         dates=test_md.Date\n",
    "#         test_md=test_md.fillna(0)\n",
    "#         test_md=test_md.drop(['Store','Type','Week_In_Year','Date','IsHoliday'],axis=1)\n",
    "#         sales_pred=models_md[sd[1]].predict(test_md)\n",
    "#         preds.append(pd.DataFrame({'Id':str(sd[0])+'_'+str(sd[1])+'_'+dates.astype('U'),'Weekly_Sales':sales_pred.astype('int')}))\n",
    "#         md_pred+=len(sales_pred)\n",
    "    \n",
    "#     test_no_md=depts_p[np.logical_not(hasMarkDown)]\n",
    "#     if len(test_no_md)>0:\n",
    "    dates=depts_p.Date\n",
    "    test_no_md=depts_p.drop(['Store','Type','Week_In_Year','Date','IsHoliday']+md_cols,axis=1)\n",
    "    sales_pred=models[sd[1]].predict(test_no_md)\n",
    "    preds.append(pd.DataFrame({'Id':str(sd[0])+'_'+str(sd[1])+'_'+dates.astype('U'),'Weekly_Sales':sales_pred.astype('int')}))\n",
    "    \n",
    "    \n",
    "result=pd.concat(preds)\n",
    "result.to_csv('output/regression/result.csv',index=False,header=True)\n",
    "\n",
    "# print(md_pred, len(test_p)-md_pred)\n",
    "'total time %ds' % int(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales1=train_md[train_md.Dept==1].Weekly_Sales\n",
    "train1=train_md[train_md.Dept==1].drop(['Store','Type','Weekly_Sales','Week_In_Year','Date','IsHoliday'],axis=1)\n",
    "gbt=GradientBoostingRegressor()\n",
    "gbt.fit(train1,sales1)\n",
    "sorted(zip(train1.columns,gbt.feature_importances_),key=lambda x:x[1],reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
