{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import arima\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "from sklearn.decomposition import PCA\n",
    "import stl\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from pipeline import *\n",
    "from onehot import *\n",
    "from util import *\n",
    "\n",
    "# load data\n",
    "train_data = pd.read_csv('raw_data/train.csv')\n",
    "test_data = pd.read_csv('raw_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dept 1\n",
      "[0, 0, 0] Number of states in statespace model must be a positive number.\n",
      "[0, 0, 1] 3154.70115585\n",
      "[0, 0, 2] 3119.75162681\n",
      "[0, 1, 0] Number of states in statespace model must be a positive number.\n",
      "[0, 1, 1] 2963.5258377\n",
      "[0, 1, 2] 2912.25018302\n",
      "[0, 2, 0] Number of states in statespace model must be a positive number.\n",
      "[0, 2, 1] 2960.47553358\n",
      "[0, 2, 2] 2937.30601857\n",
      "[1, 0, 0] 3004.58413137\n",
      "[1, 0, 1] 2982.83576536\n",
      "[1, 0, 2] 2928.89450564\n",
      "[1, 1, 0] 2984.38212874\n",
      "[1, 1, 1] 2958.31172166\n",
      "[1, 1, 2] 2917.60031697\n",
      "[1, 2, 0] 3045.1416999\n",
      "[1, 2, 1] 2961.44323238\n",
      "[1, 2, 2] 2928.3903694\n",
      "[2, 0, 0] 2982.97695981\n",
      "[2, 0, 1] 3173.66229006\n",
      "[2, 0, 2] 2933.43364036\n",
      "[2, 1, 0] 2950.93353122\n",
      "[2, 1, 1] 2941.50659312\n",
      "[2, 1, 2] 2919.85535492\n",
      "[2, 2, 0] 2997.9387122\n",
      "[2, 2, 1] 2946.13155026\n",
      "[2, 2, 2] 2921.75173224\n",
      "best %s %s [0, 1, 2] 2912.25018302\n"
     ]
    }
   ],
   "source": [
    "def select_arima_model(sales, orders, factors=None,speedup=False):\n",
    "    best_r = None\n",
    "    best_o = None\n",
    "#     min_aic=9999999999999\n",
    "    min_bic=9999999999999\n",
    "        \n",
    "    for o in orders:\n",
    "        try:\n",
    "            if speedup:\n",
    "                m=sm.tsa.statespace.SARIMAX(sales,order=o,exog=factors,\n",
    "                         simple_differencing=True, enforce_stationarity=False, enforce_invertibility=False)\n",
    "                    \n",
    "            else:\n",
    "                m=sm.tsa.statespace.SARIMAX(sales,order=o,exog=factors)\n",
    "                \n",
    "            r=m.fit(disp=False)\n",
    "            if r.bic < min_bic:\n",
    "                best_r = r\n",
    "                best_o = o\n",
    "#                 min_aic=r.aic\n",
    "                min_bic=r.bic\n",
    "            print(o,r.bic)\n",
    "\n",
    "        except Exception as e:\n",
    "            print('%s %s'% (o,e))\n",
    "#             traceback.print_exc()\n",
    "            \n",
    "    return best_r,best_o\n",
    "\n",
    "def make_orders(range_num, seq_num):\n",
    "    if seq_num == 0:\n",
    "        return [[]]\n",
    "    else:\n",
    "        orders=[]\n",
    "        sub_orders=make_orders(range_num,seq_num-1)\n",
    "        for o in sub_orders:\n",
    "            for i in range(range_num):\n",
    "                s=o.copy()\n",
    "                s.append(i)\n",
    "                orders.append(s)\n",
    "        return orders\n",
    "    \n",
    "depts=train_data.Dept.unique()\n",
    "# iterate every dept\n",
    "for dept in depts:\n",
    "    print('dept %d' % dept)\n",
    "    # transform train to dept, stores(in columns) matrix, index is date fill na with 0\n",
    "    dept_data = train_data[train_data.Dept == dept][['Date','Store','Weekly_Sales']]\n",
    "    dept_stores = pd.pivot_table(dept_data, values='Weekly_Sales', index='Date', columns='Store', fill_value=0.)\n",
    "    \n",
    "    # pca preprocess the sales between stores to filter noise ,use 12 components    \n",
    "    idxs=dept_stores.index\n",
    "    cols=dept_stores.columns\n",
    "    pca = PCA(n_components=12)\n",
    "    dept_pca = pca.fit_transform(dept_stores)\n",
    "    dept_restore = pca.inverse_transform(dept_pca)\n",
    "    dept_stores = pd.DataFrame(dept_restore,index=pd.DatetimeIndex(idxs),columns=cols)\n",
    "    \n",
    "    # iterate every store\n",
    "    for store in dept_stores.columns:\n",
    "        store_data = dept_stores.loc[:,store]\n",
    "        # stl the ts\n",
    "        de = stl.stl_decompose(store_data)\n",
    "        # select arima for trend comp\n",
    "        orders = make_orders(3,3)\n",
    "        r,o=select_arima_model(store_data.values,orders,speedup=True)\n",
    "        print('best %s %s',o,r.bic)\n",
    "        # forecast until test end date\n",
    "        # add last season comp to forecasts\n",
    "        break\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function lowess in module statsmodels.nonparametric.smoothers_lowess:\n",
      "\n",
      "lowess(endog, exog, frac=0.6666666666666666, it=3, delta=0.0, is_sorted=False, missing='drop', return_sorted=True)\n",
      "    LOWESS (Locally Weighted Scatterplot Smoothing)\n",
      "    \n",
      "    A lowess function that outs smoothed estimates of endog\n",
      "    at the given exog values from points (exog, endog)\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    endog: 1-D numpy array\n",
      "        The y-values of the observed points\n",
      "    exog: 1-D numpy array\n",
      "        The x-values of the observed points\n",
      "    frac: float\n",
      "        Between 0 and 1. The fraction of the data used\n",
      "        when estimating each y-value.\n",
      "    it: int\n",
      "        The number of residual-based reweightings\n",
      "        to perform.\n",
      "    delta: float\n",
      "        Distance within which to use linear-interpolation\n",
      "        instead of weighted regression.\n",
      "    is_sorted : bool\n",
      "        If False (default), then the data will be sorted by exog before\n",
      "        calculating lowess. If True, then it is assumed that the data is\n",
      "        already sorted by exog.\n",
      "    missing : str\n",
      "        Available options are 'none', 'drop', and 'raise'. If 'none', no nan\n",
      "        checking is done. If 'drop', any observations with nans are dropped.\n",
      "        If 'raise', an error is raised. Default is 'drop'.\n",
      "    return_sorted : bool\n",
      "        If True (default), then the returned array is sorted by exog and has\n",
      "        missing (nan or infinite) observations removed.\n",
      "        If False, then the returned array is in the same length and the same\n",
      "        sequence of observations as the input array.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    out: ndarray, float\n",
      "        The returned array is two-dimensional if return_sorted is True, and\n",
      "        one dimensional if return_sorted is False.\n",
      "        If return_sorted is True, then a numpy array with two columns. The\n",
      "        first column contains the sorted x (exog) values and the second column\n",
      "        the associated estimated y (endog) values.\n",
      "        If return_sorted is False, then only the fitted values are returned,\n",
      "        and the observations will be in the same order as the input arrays.\n",
      "    \n",
      "    Notes\n",
      "    -----\n",
      "    This lowess function implements the algorithm given in the\n",
      "    reference below using local linear estimates.\n",
      "    \n",
      "    Suppose the input data has N points. The algorithm works by\n",
      "    estimating the `smooth` y_i by taking the frac*N closest points\n",
      "    to (x_i,y_i) based on their x values and estimating y_i\n",
      "    using a weighted linear regression. The weight for (x_j,y_j)\n",
      "    is tricube function applied to abs(x_i-x_j).\n",
      "    \n",
      "    If it > 1, then further weighted local linear regressions\n",
      "    are performed, where the weights are the same as above\n",
      "    times the _lowess_bisquare function of the residuals. Each iteration\n",
      "    takes approximately the same amount of time as the original fit,\n",
      "    so these iterations are expensive. They are most useful when\n",
      "    the noise has extremely heavy tails, such as Cauchy noise.\n",
      "    Noise with less heavy-tails, such as t-distributions with df>2,\n",
      "    are less problematic. The weights downgrade the influence of\n",
      "    points with large residuals. In the extreme case, points whose\n",
      "    residuals are larger than 6 times the median absolute residual\n",
      "    are given weight 0.\n",
      "    \n",
      "    `delta` can be used to save computations. For each `x_i`, regressions\n",
      "    are skipped for points closer than `delta`. The next regression is\n",
      "    fit for the farthest point within delta of `x_i` and all points in\n",
      "    between are estimated by linearly interpolating between the two\n",
      "    regression fits.\n",
      "    \n",
      "    Judicious choice of delta can cut computation time considerably\n",
      "    for large data (N > 5000). A good choice is ``delta = 0.01 * range(exog)``.\n",
      "    \n",
      "    Some experimentation is likely required to find a good\n",
      "    choice of `frac` and `iter` for a particular dataset.\n",
      "    \n",
      "    References\n",
      "    ----------\n",
      "    Cleveland, W.S. (1979) \"Robust Locally Weighted Regression\n",
      "    and Smoothing Scatterplots\". Journal of the American Statistical\n",
      "    Association 74 (368): 829-836.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    The below allows a comparison between how different the fits from\n",
      "    lowess for different values of frac can be.\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> import statsmodels.api as sm\n",
      "    >>> lowess = sm.nonparametric.lowess\n",
      "    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\n",
      "    >>> y = np.sin(x) + np.random.normal(size=len(x))\n",
      "    >>> z = lowess(y, x)\n",
      "    >>> w = lowess(y, x, frac=1./3)\n",
      "    \n",
      "    This gives a similar comparison for when it is 0 vs not.\n",
      "    \n",
      "    >>> import numpy as np\n",
      "    >>> import scipy.stats as stats\n",
      "    >>> import statsmodels.api as sm\n",
      "    >>> lowess = sm.nonparametric.lowess\n",
      "    >>> x = np.random.uniform(low = -2*np.pi, high = 2*np.pi, size=500)\n",
      "    >>> y = np.sin(x) + stats.cauchy.rvs(size=len(x))\n",
      "    >>> z = lowess(y, x, frac= 1./3, it=0)\n",
      "    >>> w = lowess(y, x, frac=1./3)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sm.nonparametric.lowess)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
